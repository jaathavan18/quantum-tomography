{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee16bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: expt_3\n",
      "Master seed: 48\n",
      "================================================================================\n",
      "QUANTUM STATE TOMOGRAPHY EXPERIMENTS\n",
      "Output directory: expt_3\n",
      "Master seed: 48\n",
      "================================================================================\n",
      "\n",
      "Starting Priority A experiments...\n",
      "\n",
      "================================================================================\n",
      "PRIORITY A: Finite Shots + Readout Noise\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise0pct_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.080878, Val Loss=0.051716, Fidelity=0.8453\n",
      "Epoch 50: Train Loss=0.053912, Val Loss=0.050174, Fidelity=0.8618\n",
      "Epoch 100: Train Loss=0.053519, Val Loss=0.049656, Fidelity=0.8567\n",
      "Epoch 150: Train Loss=0.053370, Val Loss=0.049975, Fidelity=0.8616\n",
      "Epoch 200: Train Loss=0.053184, Val Loss=0.049899, Fidelity=0.8509\n",
      "Epoch 250: Train Loss=0.053057, Val Loss=0.049725, Fidelity=0.8553\n",
      "Early stopping at epoch 258\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8589 ± 0.1123\n",
      "  RMSE (x,y,z): (0.2300, 0.2268, 0.2173)\n",
      "  Frac > 0.95: 0.2429\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise0pct_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.086208, Val Loss=0.051733, Fidelity=0.8411\n",
      "Epoch 50: Train Loss=0.053804, Val Loss=0.050462, Fidelity=0.8614\n",
      "Epoch 100: Train Loss=0.053483, Val Loss=0.050482, Fidelity=0.8587\n",
      "Epoch 150: Train Loss=0.053296, Val Loss=0.050569, Fidelity=0.8606\n",
      "Epoch 200: Train Loss=0.053056, Val Loss=0.050518, Fidelity=0.8599\n",
      "Early stopping at epoch 210\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8594 ± 0.1137\n",
      "  RMSE (x,y,z): (0.2294, 0.2293, 0.2162)\n",
      "  Frac > 0.95: 0.2725\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise0pct_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.083643, Val Loss=0.051839, Fidelity=0.8388\n",
      "Epoch 50: Train Loss=0.054006, Val Loss=0.050409, Fidelity=0.8608\n",
      "Epoch 100: Train Loss=0.053655, Val Loss=0.050270, Fidelity=0.8596\n",
      "Early stopping at epoch 115\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8635 ± 0.1143\n",
      "  RMSE (x,y,z): (0.2280, 0.2299, 0.2149)\n",
      "  Frac > 0.95: 0.2933\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise0pct_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.128186, Val Loss=0.105134, Fidelity=0.7647\n",
      "Epoch 50: Train Loss=0.106981, Val Loss=0.103678, Fidelity=0.7703\n",
      "Epoch 100: Train Loss=0.106719, Val Loss=0.103423, Fidelity=0.7715\n",
      "Epoch 150: Train Loss=0.106709, Val Loss=0.103413, Fidelity=0.7702\n",
      "Early stopping at epoch 179\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7727 ± 0.1415\n",
      "  RMSE (x,y,z): (0.2262, 0.4547, 0.2204)\n",
      "  Frac > 0.95: 0.0855\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise0pct_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.125759, Val Loss=0.104765, Fidelity=0.7622\n",
      "Epoch 50: Train Loss=0.106454, Val Loss=0.104062, Fidelity=0.7728\n",
      "Epoch 100: Train Loss=0.106311, Val Loss=0.103830, Fidelity=0.7713\n",
      "Epoch 150: Train Loss=0.106137, Val Loss=0.103773, Fidelity=0.7724\n",
      "Epoch 200: Train Loss=0.106136, Val Loss=0.103813, Fidelity=0.7701\n",
      "Epoch 250: Train Loss=0.106014, Val Loss=0.103833, Fidelity=0.7738\n",
      "Early stopping at epoch 268\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7695 ± 0.1432\n",
      "  RMSE (x,y,z): (0.2316, 0.4633, 0.2213)\n",
      "  Frac > 0.95: 0.1068\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise0pct_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.123150, Val Loss=0.106039, Fidelity=0.7645\n",
      "Epoch 50: Train Loss=0.106235, Val Loss=0.105413, Fidelity=0.7715\n",
      "Epoch 100: Train Loss=0.106117, Val Loss=0.105482, Fidelity=0.7675\n",
      "Epoch 150: Train Loss=0.105904, Val Loss=0.105198, Fidelity=0.7731\n",
      "Early stopping at epoch 187\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7747 ± 0.1413\n",
      "  RMSE (x,y,z): (0.2268, 0.4566, 0.2185)\n",
      "  Frac > 0.95: 0.1073\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise1pct_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.085664, Val Loss=0.051102, Fidelity=0.8421\n",
      "Epoch 50: Train Loss=0.053831, Val Loss=0.049672, Fidelity=0.8566\n",
      "Epoch 100: Train Loss=0.053507, Val Loss=0.049630, Fidelity=0.8573\n",
      "Epoch 150: Train Loss=0.053261, Val Loss=0.050062, Fidelity=0.8561\n",
      "Epoch 200: Train Loss=0.053091, Val Loss=0.049777, Fidelity=0.8567\n",
      "Early stopping at epoch 216\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8555 ± 0.1112\n",
      "  RMSE (x,y,z): (0.2292, 0.2269, 0.2183)\n",
      "  Frac > 0.95: 0.2127\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise1pct_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.090040, Val Loss=0.052055, Fidelity=0.8398\n",
      "Epoch 50: Train Loss=0.053571, Val Loss=0.050884, Fidelity=0.8562\n",
      "Epoch 100: Train Loss=0.053166, Val Loss=0.050360, Fidelity=0.8553\n",
      "Epoch 150: Train Loss=0.053083, Val Loss=0.050334, Fidelity=0.8605\n",
      "Epoch 200: Train Loss=0.052704, Val Loss=0.050295, Fidelity=0.8602\n",
      "Epoch 250: Train Loss=0.052754, Val Loss=0.050109, Fidelity=0.8561\n",
      "Epoch 300: Train Loss=0.052728, Val Loss=0.050314, Fidelity=0.8585\n",
      "Early stopping at epoch 321\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8564 ± 0.1110\n",
      "  RMSE (x,y,z): (0.2289, 0.2290, 0.2158)\n",
      "  Frac > 0.95: 0.2262\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise1pct_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.080472, Val Loss=0.051660, Fidelity=0.8460\n",
      "Epoch 50: Train Loss=0.053770, Val Loss=0.050265, Fidelity=0.8582\n",
      "Epoch 100: Train Loss=0.053182, Val Loss=0.050006, Fidelity=0.8551\n",
      "Epoch 150: Train Loss=0.052985, Val Loss=0.050210, Fidelity=0.8580\n",
      "Epoch 200: Train Loss=0.052770, Val Loss=0.050050, Fidelity=0.8549\n",
      "Epoch 250: Train Loss=0.052811, Val Loss=0.050015, Fidelity=0.8550\n",
      "Epoch 300: Train Loss=0.052713, Val Loss=0.050047, Fidelity=0.8540\n",
      "Early stopping at epoch 345\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8580 ± 0.1106\n",
      "  RMSE (x,y,z): (0.2280, 0.2296, 0.2135)\n",
      "  Frac > 0.95: 0.2324\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise1pct_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.130547, Val Loss=0.104698, Fidelity=0.7614\n",
      "Epoch 50: Train Loss=0.106971, Val Loss=0.103778, Fidelity=0.7726\n",
      "Epoch 100: Train Loss=0.106742, Val Loss=0.103392, Fidelity=0.7721\n",
      "Early stopping at epoch 147\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7747 ± 0.1425\n",
      "  RMSE (x,y,z): (0.2263, 0.4547, 0.2204)\n",
      "  Frac > 0.95: 0.1036\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise1pct_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.125723, Val Loss=0.105055, Fidelity=0.7664\n",
      "Epoch 50: Train Loss=0.106442, Val Loss=0.103918, Fidelity=0.7735\n",
      "Epoch 100: Train Loss=0.106322, Val Loss=0.103801, Fidelity=0.7717\n",
      "Epoch 150: Train Loss=0.106126, Val Loss=0.103942, Fidelity=0.7746\n",
      "Early stopping at epoch 183\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7679 ± 0.1432\n",
      "  RMSE (x,y,z): (0.2314, 0.4633, 0.2211)\n",
      "  Frac > 0.95: 0.1079\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise1pct_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.128401, Val Loss=0.106142, Fidelity=0.7651\n",
      "Epoch 50: Train Loss=0.106416, Val Loss=0.105527, Fidelity=0.7669\n",
      "Epoch 100: Train Loss=0.106012, Val Loss=0.105224, Fidelity=0.7660\n",
      "Epoch 150: Train Loss=0.105982, Val Loss=0.105166, Fidelity=0.7725\n",
      "Epoch 200: Train Loss=0.105925, Val Loss=0.105245, Fidelity=0.7682\n",
      "Epoch 250: Train Loss=0.105898, Val Loss=0.105257, Fidelity=0.7682\n",
      "Epoch 300: Train Loss=0.105851, Val Loss=0.105144, Fidelity=0.7673\n",
      "Epoch 350: Train Loss=0.105862, Val Loss=0.105151, Fidelity=0.7687\n",
      "Early stopping at epoch 382\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7743 ± 0.1400\n",
      "  RMSE (x,y,z): (0.2264, 0.4566, 0.2185)\n",
      "  Frac > 0.95: 0.0922\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise5pct_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.083151, Val Loss=0.050841, Fidelity=0.8417\n",
      "Epoch 50: Train Loss=0.053945, Val Loss=0.049915, Fidelity=0.8589\n",
      "Epoch 100: Train Loss=0.053400, Val Loss=0.049768, Fidelity=0.8533\n",
      "Early stopping at epoch 135\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8572 ± 0.1125\n",
      "  RMSE (x,y,z): (0.2300, 0.2276, 0.2178)\n",
      "  Frac > 0.95: 0.2367\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise5pct_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.086641, Val Loss=0.052084, Fidelity=0.8427\n",
      "Epoch 50: Train Loss=0.053530, Val Loss=0.050531, Fidelity=0.8571\n",
      "Epoch 100: Train Loss=0.053081, Val Loss=0.050497, Fidelity=0.8613\n",
      "Epoch 150: Train Loss=0.052950, Val Loss=0.050484, Fidelity=0.8588\n",
      "Epoch 200: Train Loss=0.052751, Val Loss=0.050437, Fidelity=0.8593\n",
      "Epoch 250: Train Loss=0.052749, Val Loss=0.050129, Fidelity=0.8559\n",
      "Epoch 300: Train Loss=0.052677, Val Loss=0.050187, Fidelity=0.8584\n",
      "Epoch 350: Train Loss=0.052575, Val Loss=0.050262, Fidelity=0.8587\n",
      "Early stopping at epoch 370\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8589 ± 0.1121\n",
      "  RMSE (x,y,z): (0.2293, 0.2288, 0.2158)\n",
      "  Frac > 0.95: 0.2441\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise5pct_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.087682, Val Loss=0.052078, Fidelity=0.8402\n",
      "Epoch 50: Train Loss=0.053726, Val Loss=0.050198, Fidelity=0.8556\n",
      "Epoch 100: Train Loss=0.053370, Val Loss=0.050349, Fidelity=0.8572\n",
      "Epoch 150: Train Loss=0.053214, Val Loss=0.050270, Fidelity=0.8568\n",
      "Epoch 200: Train Loss=0.053106, Val Loss=0.050004, Fidelity=0.8568\n",
      "Epoch 250: Train Loss=0.052914, Val Loss=0.050239, Fidelity=0.8578\n",
      "Epoch 300: Train Loss=0.052830, Val Loss=0.050056, Fidelity=0.8580\n",
      "Epoch 350: Train Loss=0.052699, Val Loss=0.050203, Fidelity=0.8567\n",
      "Epoch 400: Train Loss=0.052723, Val Loss=0.049918, Fidelity=0.8545\n",
      "Epoch 450: Train Loss=0.052762, Val Loss=0.050041, Fidelity=0.8562\n",
      "Early stopping at epoch 497\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8582 ± 0.1110\n",
      "  RMSE (x,y,z): (0.2285, 0.2297, 0.2127)\n",
      "  Frac > 0.95: 0.2374\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise5pct_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.128666, Val Loss=0.104744, Fidelity=0.7577\n",
      "Epoch 50: Train Loss=0.107022, Val Loss=0.103414, Fidelity=0.7698\n",
      "Epoch 100: Train Loss=0.106733, Val Loss=0.103327, Fidelity=0.7691\n",
      "Epoch 150: Train Loss=0.106568, Val Loss=0.103342, Fidelity=0.7714\n",
      "Epoch 200: Train Loss=0.106603, Val Loss=0.103282, Fidelity=0.7700\n",
      "Epoch 250: Train Loss=0.106561, Val Loss=0.103587, Fidelity=0.7724\n",
      "Early stopping at epoch 272\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7741 ± 0.1423\n",
      "  RMSE (x,y,z): (0.2264, 0.4549, 0.2204)\n",
      "  Frac > 0.95: 0.1049\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise5pct_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.124311, Val Loss=0.104915, Fidelity=0.7648\n",
      "Epoch 50: Train Loss=0.106513, Val Loss=0.103742, Fidelity=0.7720\n",
      "Epoch 100: Train Loss=0.106109, Val Loss=0.103899, Fidelity=0.7741\n",
      "Epoch 150: Train Loss=0.106060, Val Loss=0.103812, Fidelity=0.7723\n",
      "Epoch 200: Train Loss=0.105993, Val Loss=0.103751, Fidelity=0.7747\n",
      "Epoch 250: Train Loss=0.106020, Val Loss=0.103773, Fidelity=0.7717\n",
      "Early stopping at epoch 262\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7692 ± 0.1430\n",
      "  RMSE (x,y,z): (0.2313, 0.4633, 0.2216)\n",
      "  Frac > 0.95: 0.1064\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots10_noise5pct_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.124692, Val Loss=0.106103, Fidelity=0.7606\n",
      "Epoch 50: Train Loss=0.106289, Val Loss=0.105296, Fidelity=0.7687\n",
      "Epoch 100: Train Loss=0.106159, Val Loss=0.105207, Fidelity=0.7725\n",
      "Epoch 150: Train Loss=0.106089, Val Loss=0.105292, Fidelity=0.7670\n",
      "Early stopping at epoch 195\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7725 ± 0.1400\n",
      "  RMSE (x,y,z): (0.2269, 0.4567, 0.2187)\n",
      "  Frac > 0.95: 0.0838\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise0pct_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.050888, Val Loss=0.008328, Fidelity=0.9074\n",
      "Epoch 50: Train Loss=0.008944, Val Loss=0.006558, Fidelity=0.9228\n",
      "Epoch 100: Train Loss=0.008554, Val Loss=0.006512, Fidelity=0.9211\n",
      "Epoch 150: Train Loss=0.008398, Val Loss=0.006503, Fidelity=0.9218\n",
      "Epoch 200: Train Loss=0.008329, Val Loss=0.006487, Fidelity=0.9224\n",
      "Epoch 250: Train Loss=0.008262, Val Loss=0.006459, Fidelity=0.9206\n",
      "Epoch 300: Train Loss=0.008250, Val Loss=0.006394, Fidelity=0.9212\n",
      "Epoch 350: Train Loss=0.008195, Val Loss=0.006416, Fidelity=0.9210\n",
      "Epoch 400: Train Loss=0.008212, Val Loss=0.006410, Fidelity=0.9208\n",
      "Early stopping at epoch 448\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9237 ± 0.1212\n",
      "  RMSE (x,y,z): (0.0829, 0.0829, 0.0716)\n",
      "  Frac > 0.95: 0.6959\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise0pct_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.044144, Val Loss=0.007894, Fidelity=0.9073\n",
      "Epoch 50: Train Loss=0.008986, Val Loss=0.006455, Fidelity=0.9214\n",
      "Epoch 100: Train Loss=0.008624, Val Loss=0.006353, Fidelity=0.9217\n",
      "Epoch 150: Train Loss=0.008510, Val Loss=0.006180, Fidelity=0.9216\n",
      "Epoch 200: Train Loss=0.008433, Val Loss=0.006268, Fidelity=0.9220\n",
      "Early stopping at epoch 250\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9212 ± 0.1220\n",
      "  RMSE (x,y,z): (0.0835, 0.0822, 0.0723)\n",
      "  Frac > 0.95: 0.6860\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise0pct_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.044594, Val Loss=0.007976, Fidelity=0.9059\n",
      "Epoch 50: Train Loss=0.008978, Val Loss=0.006506, Fidelity=0.9201\n",
      "Epoch 100: Train Loss=0.008630, Val Loss=0.006444, Fidelity=0.9217\n",
      "Epoch 150: Train Loss=0.008475, Val Loss=0.006487, Fidelity=0.9202\n",
      "Epoch 200: Train Loss=0.008449, Val Loss=0.006434, Fidelity=0.9201\n",
      "Epoch 250: Train Loss=0.008413, Val Loss=0.006448, Fidelity=0.9208\n",
      "Epoch 300: Train Loss=0.008321, Val Loss=0.006367, Fidelity=0.9210\n",
      "Epoch 350: Train Loss=0.008302, Val Loss=0.006323, Fidelity=0.9205\n",
      "Early stopping at epoch 376\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9230 ± 0.1213\n",
      "  RMSE (x,y,z): (0.0830, 0.0834, 0.0745)\n",
      "  Frac > 0.95: 0.6950\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise0pct_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.103709, Val Loss=0.073893, Fidelity=0.8111\n",
      "Epoch 50: Train Loss=0.076989, Val Loss=0.073270, Fidelity=0.8251\n",
      "Epoch 100: Train Loss=0.076916, Val Loss=0.073035, Fidelity=0.8214\n",
      "Early stopping at epoch 138\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8211 ± 0.1558\n",
      "  RMSE (x,y,z): (0.0846, 0.4580, 0.0780)\n",
      "  Frac > 0.95: 0.3448\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise0pct_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.099825, Val Loss=0.075041, Fidelity=0.8063\n",
      "Epoch 50: Train Loss=0.077196, Val Loss=0.074078, Fidelity=0.8204\n",
      "Epoch 100: Train Loss=0.077023, Val Loss=0.074013, Fidelity=0.8196\n",
      "Epoch 150: Train Loss=0.077026, Val Loss=0.074045, Fidelity=0.8190\n",
      "Epoch 200: Train Loss=0.076928, Val Loss=0.074032, Fidelity=0.8177\n",
      "Epoch 250: Train Loss=0.076920, Val Loss=0.074084, Fidelity=0.8196\n",
      "Epoch 300: Train Loss=0.076910, Val Loss=0.074058, Fidelity=0.8192\n",
      "Epoch 350: Train Loss=0.076909, Val Loss=0.074062, Fidelity=0.8186\n",
      "Early stopping at epoch 355\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8157 ± 0.1557\n",
      "  RMSE (x,y,z): (0.0858, 0.4635, 0.0759)\n",
      "  Frac > 0.95: 0.3229\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise0pct_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.101663, Val Loss=0.075910, Fidelity=0.8105\n",
      "Epoch 50: Train Loss=0.076561, Val Loss=0.075216, Fidelity=0.8138\n",
      "Epoch 100: Train Loss=0.076493, Val Loss=0.075201, Fidelity=0.8163\n",
      "Epoch 150: Train Loss=0.076366, Val Loss=0.075139, Fidelity=0.8183\n",
      "Epoch 200: Train Loss=0.076284, Val Loss=0.075135, Fidelity=0.8154\n",
      "Epoch 250: Train Loss=0.076355, Val Loss=0.075153, Fidelity=0.8181\n",
      "Epoch 300: Train Loss=0.076328, Val Loss=0.075189, Fidelity=0.8166\n",
      "Epoch 350: Train Loss=0.076250, Val Loss=0.075131, Fidelity=0.8162\n",
      "Early stopping at epoch 385\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8179 ± 0.1555\n",
      "  RMSE (x,y,z): (0.0853, 0.4621, 0.0772)\n",
      "  Frac > 0.95: 0.3301\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise1pct_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.046951, Val Loss=0.008217, Fidelity=0.9056\n",
      "Epoch 50: Train Loss=0.008934, Val Loss=0.006584, Fidelity=0.9205\n",
      "Epoch 100: Train Loss=0.008639, Val Loss=0.006481, Fidelity=0.9212\n",
      "Epoch 150: Train Loss=0.008491, Val Loss=0.006542, Fidelity=0.9222\n",
      "Epoch 200: Train Loss=0.008363, Val Loss=0.006377, Fidelity=0.9212\n",
      "Epoch 250: Train Loss=0.008342, Val Loss=0.006450, Fidelity=0.9217\n",
      "Epoch 300: Train Loss=0.008290, Val Loss=0.006454, Fidelity=0.9206\n",
      "Epoch 350: Train Loss=0.008276, Val Loss=0.006408, Fidelity=0.9211\n",
      "Epoch 400: Train Loss=0.008255, Val Loss=0.006416, Fidelity=0.9204\n",
      "Epoch 450: Train Loss=0.008207, Val Loss=0.006415, Fidelity=0.9210\n",
      "Early stopping at epoch 492\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9236 ± 0.1211\n",
      "  RMSE (x,y,z): (0.0833, 0.0828, 0.0721)\n",
      "  Frac > 0.95: 0.6954\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise1pct_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.044748, Val Loss=0.007991, Fidelity=0.9078\n",
      "Epoch 50: Train Loss=0.009018, Val Loss=0.006397, Fidelity=0.9219\n",
      "Epoch 100: Train Loss=0.008637, Val Loss=0.006395, Fidelity=0.9218\n",
      "Epoch 150: Train Loss=0.008519, Val Loss=0.006278, Fidelity=0.9204\n",
      "Epoch 200: Train Loss=0.008449, Val Loss=0.006282, Fidelity=0.9211\n",
      "Epoch 250: Train Loss=0.008435, Val Loss=0.006236, Fidelity=0.9216\n",
      "Early stopping at epoch 254\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9218 ± 0.1215\n",
      "  RMSE (x,y,z): (0.0826, 0.0827, 0.0721)\n",
      "  Frac > 0.95: 0.6871\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise1pct_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.050475, Val Loss=0.008229, Fidelity=0.9070\n",
      "Epoch 50: Train Loss=0.009022, Val Loss=0.006544, Fidelity=0.9205\n",
      "Epoch 100: Train Loss=0.008609, Val Loss=0.006442, Fidelity=0.9207\n",
      "Epoch 150: Train Loss=0.008496, Val Loss=0.006400, Fidelity=0.9200\n",
      "Epoch 200: Train Loss=0.008441, Val Loss=0.006361, Fidelity=0.9206\n",
      "Epoch 250: Train Loss=0.008379, Val Loss=0.006398, Fidelity=0.9210\n",
      "Epoch 300: Train Loss=0.008330, Val Loss=0.006384, Fidelity=0.9203\n",
      "Epoch 350: Train Loss=0.008291, Val Loss=0.006375, Fidelity=0.9204\n",
      "Epoch 400: Train Loss=0.008306, Val Loss=0.006363, Fidelity=0.9198\n",
      "Epoch 450: Train Loss=0.008273, Val Loss=0.006367, Fidelity=0.9205\n",
      "Early stopping at epoch 488\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9221 ± 0.1223\n",
      "  RMSE (x,y,z): (0.0826, 0.0828, 0.0732)\n",
      "  Frac > 0.95: 0.6929\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise1pct_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.100148, Val Loss=0.074036, Fidelity=0.8107\n",
      "Epoch 50: Train Loss=0.076970, Val Loss=0.073090, Fidelity=0.8227\n",
      "Epoch 100: Train Loss=0.076873, Val Loss=0.072998, Fidelity=0.8233\n",
      "Epoch 150: Train Loss=0.076699, Val Loss=0.073062, Fidelity=0.8213\n",
      "Epoch 200: Train Loss=0.076656, Val Loss=0.073042, Fidelity=0.8220\n",
      "Epoch 250: Train Loss=0.076703, Val Loss=0.073041, Fidelity=0.8206\n",
      "Early stopping at epoch 294\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8169 ± 0.1547\n",
      "  RMSE (x,y,z): (0.0844, 0.4580, 0.0761)\n",
      "  Frac > 0.95: 0.3239\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise1pct_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.102583, Val Loss=0.075254, Fidelity=0.8137\n",
      "Epoch 50: Train Loss=0.077271, Val Loss=0.074227, Fidelity=0.8217\n",
      "Epoch 100: Train Loss=0.077153, Val Loss=0.074110, Fidelity=0.8200\n",
      "Epoch 150: Train Loss=0.077051, Val Loss=0.074098, Fidelity=0.8181\n",
      "Epoch 200: Train Loss=0.076954, Val Loss=0.074026, Fidelity=0.8195\n",
      "Epoch 250: Train Loss=0.076952, Val Loss=0.074072, Fidelity=0.8207\n",
      "Early stopping at epoch 299\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8186 ± 0.1559\n",
      "  RMSE (x,y,z): (0.0860, 0.4632, 0.0770)\n",
      "  Frac > 0.95: 0.3324\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise1pct_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.100007, Val Loss=0.075965, Fidelity=0.8068\n",
      "Epoch 50: Train Loss=0.076573, Val Loss=0.075201, Fidelity=0.8173\n",
      "Epoch 100: Train Loss=0.076364, Val Loss=0.075172, Fidelity=0.8167\n",
      "Epoch 150: Train Loss=0.076299, Val Loss=0.075182, Fidelity=0.8161\n",
      "Epoch 200: Train Loss=0.076321, Val Loss=0.075136, Fidelity=0.8169\n",
      "Epoch 250: Train Loss=0.076292, Val Loss=0.075170, Fidelity=0.8166\n",
      "Epoch 300: Train Loss=0.076264, Val Loss=0.075117, Fidelity=0.8151\n",
      "Epoch 350: Train Loss=0.076277, Val Loss=0.075184, Fidelity=0.8178\n",
      "Epoch 400: Train Loss=0.076235, Val Loss=0.075162, Fidelity=0.8166\n",
      "Epoch 450: Train Loss=0.076222, Val Loss=0.075125, Fidelity=0.8147\n",
      "Early stopping at epoch 459\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8170 ± 0.1552\n",
      "  RMSE (x,y,z): (0.0856, 0.4621, 0.0766)\n",
      "  Frac > 0.95: 0.3272\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise5pct_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.049471, Val Loss=0.008063, Fidelity=0.9055\n",
      "Epoch 50: Train Loss=0.008936, Val Loss=0.006585, Fidelity=0.9205\n",
      "Epoch 100: Train Loss=0.008633, Val Loss=0.006485, Fidelity=0.9216\n",
      "Epoch 150: Train Loss=0.008364, Val Loss=0.006429, Fidelity=0.9216\n",
      "Epoch 200: Train Loss=0.008335, Val Loss=0.006541, Fidelity=0.9209\n",
      "Epoch 250: Train Loss=0.008232, Val Loss=0.006461, Fidelity=0.9207\n",
      "Epoch 300: Train Loss=0.008190, Val Loss=0.006548, Fidelity=0.9215\n",
      "Early stopping at epoch 324\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9235 ± 0.1211\n",
      "  RMSE (x,y,z): (0.0836, 0.0823, 0.0731)\n",
      "  Frac > 0.95: 0.6957\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise5pct_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.044067, Val Loss=0.007934, Fidelity=0.9051\n",
      "Epoch 50: Train Loss=0.008971, Val Loss=0.006507, Fidelity=0.9202\n",
      "Epoch 100: Train Loss=0.008689, Val Loss=0.006342, Fidelity=0.9206\n",
      "Epoch 150: Train Loss=0.008607, Val Loss=0.006331, Fidelity=0.9214\n",
      "Epoch 200: Train Loss=0.008499, Val Loss=0.006250, Fidelity=0.9204\n",
      "Epoch 250: Train Loss=0.008434, Val Loss=0.006311, Fidelity=0.9209\n",
      "Early stopping at epoch 274\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9220 ± 0.1215\n",
      "  RMSE (x,y,z): (0.0830, 0.0824, 0.0728)\n",
      "  Frac > 0.95: 0.6883\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise5pct_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.044872, Val Loss=0.008189, Fidelity=0.9047\n",
      "Epoch 50: Train Loss=0.008939, Val Loss=0.006492, Fidelity=0.9203\n",
      "Epoch 100: Train Loss=0.008618, Val Loss=0.006379, Fidelity=0.9213\n",
      "Epoch 150: Train Loss=0.008522, Val Loss=0.006508, Fidelity=0.9214\n",
      "Epoch 200: Train Loss=0.008422, Val Loss=0.006387, Fidelity=0.9202\n",
      "Epoch 250: Train Loss=0.008375, Val Loss=0.006370, Fidelity=0.9213\n",
      "Epoch 300: Train Loss=0.008350, Val Loss=0.006391, Fidelity=0.9200\n",
      "Early stopping at epoch 343\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9236 ± 0.1207\n",
      "  RMSE (x,y,z): (0.0824, 0.0835, 0.0735)\n",
      "  Frac > 0.95: 0.6963\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise5pct_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.107126, Val Loss=0.073985, Fidelity=0.8122\n",
      "Epoch 50: Train Loss=0.077016, Val Loss=0.073115, Fidelity=0.8229\n",
      "Epoch 100: Train Loss=0.076892, Val Loss=0.073038, Fidelity=0.8217\n",
      "Epoch 150: Train Loss=0.076779, Val Loss=0.073082, Fidelity=0.8225\n",
      "Epoch 200: Train Loss=0.076833, Val Loss=0.073066, Fidelity=0.8220\n",
      "Early stopping at epoch 201\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8197 ± 0.1553\n",
      "  RMSE (x,y,z): (0.0849, 0.4580, 0.0771)\n",
      "  Frac > 0.95: 0.3377\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise5pct_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.101144, Val Loss=0.075477, Fidelity=0.8094\n",
      "Epoch 50: Train Loss=0.077339, Val Loss=0.074077, Fidelity=0.8180\n",
      "Epoch 100: Train Loss=0.077074, Val Loss=0.074065, Fidelity=0.8187\n",
      "Epoch 150: Train Loss=0.077015, Val Loss=0.074103, Fidelity=0.8210\n",
      "Epoch 200: Train Loss=0.076996, Val Loss=0.074063, Fidelity=0.8183\n",
      "Epoch 250: Train Loss=0.077033, Val Loss=0.074022, Fidelity=0.8184\n",
      "Epoch 300: Train Loss=0.076993, Val Loss=0.074050, Fidelity=0.8174\n",
      "Early stopping at epoch 349\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8144 ± 0.1558\n",
      "  RMSE (x,y,z): (0.0868, 0.4632, 0.0759)\n",
      "  Frac > 0.95: 0.3210\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots100_noise5pct_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.099667, Val Loss=0.075851, Fidelity=0.8047\n",
      "Epoch 50: Train Loss=0.076613, Val Loss=0.075188, Fidelity=0.8169\n",
      "Epoch 100: Train Loss=0.076515, Val Loss=0.075228, Fidelity=0.8169\n",
      "Epoch 150: Train Loss=0.076307, Val Loss=0.075203, Fidelity=0.8150\n",
      "Epoch 200: Train Loss=0.076329, Val Loss=0.075238, Fidelity=0.8142\n",
      "Epoch 250: Train Loss=0.076324, Val Loss=0.075152, Fidelity=0.8158\n",
      "Early stopping at epoch 282\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8170 ± 0.1551\n",
      "  RMSE (x,y,z): (0.0856, 0.4619, 0.0758)\n",
      "  Frac > 0.95: 0.3255\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise0pct_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.041024, Val Loss=0.001998, Fidelity=0.9178\n",
      "Epoch 50: Train Loss=0.003188, Val Loss=0.001093, Fidelity=0.9258\n",
      "Epoch 100: Train Loss=0.002909, Val Loss=0.000972, Fidelity=0.9254\n",
      "Epoch 150: Train Loss=0.002804, Val Loss=0.000964, Fidelity=0.9250\n",
      "Epoch 200: Train Loss=0.002739, Val Loss=0.000908, Fidelity=0.9248\n",
      "Early stopping at epoch 248\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9248 ± 0.1235\n",
      "  RMSE (x,y,z): (0.0315, 0.0318, 0.0272)\n",
      "  Frac > 0.95: 0.7005\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise0pct_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.039304, Val Loss=0.002142, Fidelity=0.9132\n",
      "Epoch 50: Train Loss=0.003096, Val Loss=0.001012, Fidelity=0.9243\n",
      "Epoch 100: Train Loss=0.002827, Val Loss=0.000968, Fidelity=0.9240\n",
      "Epoch 150: Train Loss=0.002754, Val Loss=0.000881, Fidelity=0.9240\n",
      "Epoch 200: Train Loss=0.002672, Val Loss=0.000918, Fidelity=0.9242\n",
      "Epoch 250: Train Loss=0.002652, Val Loss=0.000883, Fidelity=0.9238\n",
      "Epoch 300: Train Loss=0.002622, Val Loss=0.000865, Fidelity=0.9237\n",
      "Early stopping at epoch 307\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9245 ± 0.1232\n",
      "  RMSE (x,y,z): (0.0299, 0.0306, 0.0268)\n",
      "  Frac > 0.95: 0.6986\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise0pct_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.041470, Val Loss=0.002271, Fidelity=0.9160\n",
      "Epoch 50: Train Loss=0.003221, Val Loss=0.000985, Fidelity=0.9251\n",
      "Epoch 100: Train Loss=0.002882, Val Loss=0.000976, Fidelity=0.9244\n",
      "Epoch 150: Train Loss=0.002807, Val Loss=0.000929, Fidelity=0.9246\n",
      "Epoch 200: Train Loss=0.002745, Val Loss=0.000940, Fidelity=0.9241\n",
      "Epoch 250: Train Loss=0.002723, Val Loss=0.000914, Fidelity=0.9240\n",
      "Epoch 300: Train Loss=0.002701, Val Loss=0.000907, Fidelity=0.9233\n",
      "Epoch 350: Train Loss=0.002706, Val Loss=0.000937, Fidelity=0.9233\n",
      "Epoch 400: Train Loss=0.002652, Val Loss=0.000891, Fidelity=0.9236\n",
      "Early stopping at epoch 445\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9242 ± 0.1238\n",
      "  RMSE (x,y,z): (0.0301, 0.0307, 0.0270)\n",
      "  Frac > 0.95: 0.6999\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise0pct_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.095761, Val Loss=0.072388, Fidelity=0.8106\n",
      "Epoch 50: Train Loss=0.073051, Val Loss=0.071627, Fidelity=0.8191\n",
      "Epoch 100: Train Loss=0.072839, Val Loss=0.071676, Fidelity=0.8201\n",
      "Epoch 150: Train Loss=0.072858, Val Loss=0.071670, Fidelity=0.8211\n",
      "Epoch 200: Train Loss=0.072812, Val Loss=0.071644, Fidelity=0.8198\n",
      "Epoch 250: Train Loss=0.072769, Val Loss=0.071632, Fidelity=0.8207\n",
      "Early stopping at epoch 276\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8241 ± 0.1564\n",
      "  RMSE (x,y,z): (0.0314, 0.4617, 0.0296)\n",
      "  Frac > 0.95: 0.3556\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise0pct_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.097205, Val Loss=0.072363, Fidelity=0.8129\n",
      "Epoch 50: Train Loss=0.073288, Val Loss=0.071580, Fidelity=0.8225\n",
      "Epoch 100: Train Loss=0.073161, Val Loss=0.071541, Fidelity=0.8218\n",
      "Epoch 150: Train Loss=0.073213, Val Loss=0.071499, Fidelity=0.8205\n",
      "Epoch 200: Train Loss=0.073087, Val Loss=0.071569, Fidelity=0.8195\n",
      "Epoch 250: Train Loss=0.073068, Val Loss=0.071455, Fidelity=0.8209\n",
      "Epoch 300: Train Loss=0.073040, Val Loss=0.071480, Fidelity=0.8219\n",
      "Epoch 350: Train Loss=0.073094, Val Loss=0.071494, Fidelity=0.8225\n",
      "Epoch 400: Train Loss=0.073096, Val Loss=0.071441, Fidelity=0.8219\n",
      "Epoch 450: Train Loss=0.073117, Val Loss=0.071472, Fidelity=0.8206\n",
      "Epoch 500: Train Loss=0.073019, Val Loss=0.071519, Fidelity=0.8208\n",
      "Epoch 550: Train Loss=0.073032, Val Loss=0.071437, Fidelity=0.8212\n",
      "Early stopping at epoch 560\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8202 ± 0.1578\n",
      "  RMSE (x,y,z): (0.0311, 0.4613, 0.0291)\n",
      "  Frac > 0.95: 0.3536\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise0pct_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.102077, Val Loss=0.073070, Fidelity=0.8077\n",
      "Epoch 50: Train Loss=0.073052, Val Loss=0.072370, Fidelity=0.8188\n",
      "Epoch 100: Train Loss=0.073020, Val Loss=0.072315, Fidelity=0.8179\n",
      "Epoch 150: Train Loss=0.072981, Val Loss=0.072348, Fidelity=0.8191\n",
      "Epoch 200: Train Loss=0.072937, Val Loss=0.072364, Fidelity=0.8161\n",
      "Early stopping at epoch 204\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8224 ± 0.1561\n",
      "  RMSE (x,y,z): (0.0309, 0.4584, 0.0281)\n",
      "  Frac > 0.95: 0.3495\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise1pct_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.043091, Val Loss=0.002191, Fidelity=0.9163\n",
      "Epoch 50: Train Loss=0.003149, Val Loss=0.001027, Fidelity=0.9257\n",
      "Epoch 100: Train Loss=0.002865, Val Loss=0.000951, Fidelity=0.9246\n",
      "Epoch 150: Train Loss=0.002761, Val Loss=0.000927, Fidelity=0.9249\n",
      "Epoch 200: Train Loss=0.002708, Val Loss=0.000896, Fidelity=0.9255\n",
      "Epoch 250: Train Loss=0.002677, Val Loss=0.000941, Fidelity=0.9254\n",
      "Epoch 300: Train Loss=0.002669, Val Loss=0.000921, Fidelity=0.9256\n",
      "Epoch 350: Train Loss=0.002666, Val Loss=0.000849, Fidelity=0.9251\n",
      "Epoch 400: Train Loss=0.002672, Val Loss=0.000904, Fidelity=0.9256\n",
      "Early stopping at epoch 450\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9250 ± 0.1232\n",
      "  RMSE (x,y,z): (0.0307, 0.0313, 0.0278)\n",
      "  Frac > 0.95: 0.7005\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise1pct_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.039965, Val Loss=0.002152, Fidelity=0.9162\n",
      "Epoch 50: Train Loss=0.003094, Val Loss=0.000947, Fidelity=0.9249\n",
      "Epoch 100: Train Loss=0.002834, Val Loss=0.000920, Fidelity=0.9241\n",
      "Epoch 150: Train Loss=0.002723, Val Loss=0.000951, Fidelity=0.9244\n",
      "Epoch 200: Train Loss=0.002670, Val Loss=0.000904, Fidelity=0.9242\n",
      "Epoch 250: Train Loss=0.002629, Val Loss=0.000867, Fidelity=0.9237\n",
      "Epoch 300: Train Loss=0.002637, Val Loss=0.000958, Fidelity=0.9230\n",
      "Epoch 350: Train Loss=0.002620, Val Loss=0.000832, Fidelity=0.9237\n",
      "Epoch 400: Train Loss=0.002601, Val Loss=0.000892, Fidelity=0.9233\n",
      "Epoch 450: Train Loss=0.002579, Val Loss=0.000890, Fidelity=0.9231\n",
      "Early stopping at epoch 453\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9236 ± 0.1241\n",
      "  RMSE (x,y,z): (0.0307, 0.0307, 0.0261)\n",
      "  Frac > 0.95: 0.6986\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise1pct_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.041944, Val Loss=0.002167, Fidelity=0.9165\n",
      "Epoch 50: Train Loss=0.003136, Val Loss=0.000954, Fidelity=0.9245\n",
      "Epoch 100: Train Loss=0.002845, Val Loss=0.000942, Fidelity=0.9254\n",
      "Epoch 150: Train Loss=0.002764, Val Loss=0.000940, Fidelity=0.9249\n",
      "Epoch 200: Train Loss=0.002683, Val Loss=0.000913, Fidelity=0.9253\n",
      "Epoch 250: Train Loss=0.002636, Val Loss=0.000921, Fidelity=0.9241\n",
      "Epoch 300: Train Loss=0.002620, Val Loss=0.000898, Fidelity=0.9240\n",
      "Early stopping at epoch 313\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9248 ± 0.1231\n",
      "  RMSE (x,y,z): (0.0339, 0.0317, 0.0276)\n",
      "  Frac > 0.95: 0.6999\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise1pct_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.095378, Val Loss=0.072392, Fidelity=0.8118\n",
      "Epoch 50: Train Loss=0.073095, Val Loss=0.071685, Fidelity=0.8217\n",
      "Epoch 100: Train Loss=0.072977, Val Loss=0.071669, Fidelity=0.8213\n",
      "Epoch 150: Train Loss=0.072911, Val Loss=0.071641, Fidelity=0.8208\n",
      "Epoch 200: Train Loss=0.072930, Val Loss=0.071656, Fidelity=0.8207\n",
      "Epoch 250: Train Loss=0.072906, Val Loss=0.071668, Fidelity=0.8205\n",
      "Early stopping at epoch 260\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8231 ± 0.1562\n",
      "  RMSE (x,y,z): (0.0314, 0.4617, 0.0276)\n",
      "  Frac > 0.95: 0.3527\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise1pct_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.101770, Val Loss=0.072512, Fidelity=0.8122\n",
      "Epoch 50: Train Loss=0.073190, Val Loss=0.071569, Fidelity=0.8226\n",
      "Epoch 100: Train Loss=0.073114, Val Loss=0.071505, Fidelity=0.8213\n",
      "Epoch 150: Train Loss=0.073130, Val Loss=0.071548, Fidelity=0.8219\n",
      "Epoch 200: Train Loss=0.073073, Val Loss=0.071484, Fidelity=0.8217\n",
      "Early stopping at epoch 201\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8214 ± 0.1579\n",
      "  RMSE (x,y,z): (0.0316, 0.4613, 0.0285)\n",
      "  Frac > 0.95: 0.3545\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise1pct_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.097181, Val Loss=0.073008, Fidelity=0.8108\n",
      "Epoch 50: Train Loss=0.073066, Val Loss=0.072368, Fidelity=0.8164\n",
      "Epoch 100: Train Loss=0.072906, Val Loss=0.072339, Fidelity=0.8182\n",
      "Epoch 150: Train Loss=0.072904, Val Loss=0.072300, Fidelity=0.8172\n",
      "Early stopping at epoch 196\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8237 ± 0.1561\n",
      "  RMSE (x,y,z): (0.0318, 0.4583, 0.0284)\n",
      "  Frac > 0.95: 0.3552\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise5pct_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.045791, Val Loss=0.002081, Fidelity=0.9158\n",
      "Epoch 50: Train Loss=0.003050, Val Loss=0.001030, Fidelity=0.9252\n",
      "Epoch 100: Train Loss=0.002790, Val Loss=0.000941, Fidelity=0.9252\n",
      "Epoch 150: Train Loss=0.002713, Val Loss=0.000964, Fidelity=0.9250\n",
      "Epoch 200: Train Loss=0.002634, Val Loss=0.000999, Fidelity=0.9243\n",
      "Early stopping at epoch 227\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9244 ± 0.1240\n",
      "  RMSE (x,y,z): (0.0325, 0.0325, 0.0277)\n",
      "  Frac > 0.95: 0.7005\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise5pct_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.042638, Val Loss=0.002032, Fidelity=0.9164\n",
      "Epoch 50: Train Loss=0.003142, Val Loss=0.000976, Fidelity=0.9245\n",
      "Epoch 100: Train Loss=0.002867, Val Loss=0.000992, Fidelity=0.9239\n",
      "Epoch 150: Train Loss=0.002768, Val Loss=0.000938, Fidelity=0.9246\n",
      "Epoch 200: Train Loss=0.002726, Val Loss=0.000906, Fidelity=0.9237\n",
      "Early stopping at epoch 240\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9252 ± 0.1225\n",
      "  RMSE (x,y,z): (0.0308, 0.0316, 0.0279)\n",
      "  Frac > 0.95: 0.6986\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise5pct_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.043097, Val Loss=0.002134, Fidelity=0.9154\n",
      "Epoch 50: Train Loss=0.003323, Val Loss=0.001077, Fidelity=0.9250\n",
      "Epoch 100: Train Loss=0.002977, Val Loss=0.000947, Fidelity=0.9249\n",
      "Epoch 150: Train Loss=0.002860, Val Loss=0.000958, Fidelity=0.9238\n",
      "Epoch 200: Train Loss=0.002838, Val Loss=0.000940, Fidelity=0.9231\n",
      "Epoch 250: Train Loss=0.002751, Val Loss=0.000938, Fidelity=0.9234\n",
      "Epoch 300: Train Loss=0.002738, Val Loss=0.000901, Fidelity=0.9237\n",
      "Epoch 350: Train Loss=0.002742, Val Loss=0.000946, Fidelity=0.9232\n",
      "Epoch 400: Train Loss=0.002717, Val Loss=0.000939, Fidelity=0.9230\n",
      "Epoch 450: Train Loss=0.002687, Val Loss=0.000909, Fidelity=0.9239\n",
      "Epoch 500: Train Loss=0.002659, Val Loss=0.000882, Fidelity=0.9231\n",
      "Epoch 550: Train Loss=0.002660, Val Loss=0.000865, Fidelity=0.9239\n",
      "Early stopping at epoch 588\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9232 ± 0.1251\n",
      "  RMSE (x,y,z): (0.0310, 0.0310, 0.0282)\n",
      "  Frac > 0.95: 0.6999\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise5pct_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.097659, Val Loss=0.072365, Fidelity=0.8114\n",
      "Epoch 50: Train Loss=0.073092, Val Loss=0.071685, Fidelity=0.8206\n",
      "Epoch 100: Train Loss=0.073000, Val Loss=0.071710, Fidelity=0.8208\n",
      "Epoch 150: Train Loss=0.072904, Val Loss=0.071692, Fidelity=0.8198\n",
      "Epoch 200: Train Loss=0.072915, Val Loss=0.071658, Fidelity=0.8196\n",
      "Epoch 250: Train Loss=0.072823, Val Loss=0.071646, Fidelity=0.8180\n",
      "Epoch 300: Train Loss=0.072831, Val Loss=0.071644, Fidelity=0.8186\n",
      "Epoch 350: Train Loss=0.072865, Val Loss=0.071625, Fidelity=0.8192\n",
      "Early stopping at epoch 379\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8209 ± 0.1569\n",
      "  RMSE (x,y,z): (0.0309, 0.4616, 0.0305)\n",
      "  Frac > 0.95: 0.3515\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise5pct_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.100399, Val Loss=0.072439, Fidelity=0.8113\n",
      "Epoch 50: Train Loss=0.073453, Val Loss=0.071623, Fidelity=0.8229\n",
      "Epoch 100: Train Loss=0.073268, Val Loss=0.071499, Fidelity=0.8212\n",
      "Epoch 150: Train Loss=0.073323, Val Loss=0.071602, Fidelity=0.8224\n",
      "Epoch 200: Train Loss=0.073237, Val Loss=0.071552, Fidelity=0.8216\n",
      "Epoch 250: Train Loss=0.073237, Val Loss=0.071541, Fidelity=0.8189\n",
      "Epoch 300: Train Loss=0.073174, Val Loss=0.071518, Fidelity=0.8187\n",
      "Early stopping at epoch 304\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8206 ± 0.1575\n",
      "  RMSE (x,y,z): (0.0319, 0.4613, 0.0285)\n",
      "  Frac > 0.95: 0.3541\n",
      "\n",
      "============================================================\n",
      "Running: PriorityA_shots1000_noise5pct_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.099703, Val Loss=0.073430, Fidelity=0.8092\n",
      "Epoch 50: Train Loss=0.073139, Val Loss=0.072399, Fidelity=0.8190\n",
      "Epoch 100: Train Loss=0.073084, Val Loss=0.072364, Fidelity=0.8171\n",
      "Epoch 150: Train Loss=0.072991, Val Loss=0.072390, Fidelity=0.8177\n",
      "Epoch 200: Train Loss=0.072993, Val Loss=0.072363, Fidelity=0.8165\n",
      "Epoch 250: Train Loss=0.072925, Val Loss=0.072350, Fidelity=0.8174\n",
      "Early stopping at epoch 282\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8214 ± 0.1570\n",
      "  RMSE (x,y,z): (0.0325, 0.4583, 0.0291)\n",
      "  Frac > 0.95: 0.3526\n",
      "\n",
      "Priority A completed: 54 experiments\n",
      "\n",
      "Starting Priority B experiments...\n",
      "\n",
      "================================================================================\n",
      "PRIORITY B: Pure vs Mixed vs Near-Pure\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.046404, Val Loss=0.001095, Fidelity=0.9897\n",
      "Epoch 50: Train Loss=0.001779, Val Loss=0.000208, Fidelity=0.9998\n",
      "Epoch 100: Train Loss=0.001610, Val Loss=0.000194, Fidelity=0.9999\n",
      "Epoch 150: Train Loss=0.001556, Val Loss=0.000220, Fidelity=0.9998\n",
      "Epoch 200: Train Loss=0.001470, Val Loss=0.000180, Fidelity=0.9999\n",
      "Epoch 250: Train Loss=0.001442, Val Loss=0.000216, Fidelity=0.9998\n",
      "Epoch 300: Train Loss=0.001413, Val Loss=0.000180, Fidelity=0.9999\n",
      "Early stopping at epoch 301\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9999 ± 0.0001\n",
      "  RMSE (x,y,z): (0.0139, 0.0145, 0.0128)\n",
      "  Frac > 0.95: 1.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.045308, Val Loss=0.000902, Fidelity=0.9906\n",
      "Epoch 50: Train Loss=0.001746, Val Loss=0.000206, Fidelity=0.9998\n",
      "Epoch 100: Train Loss=0.001534, Val Loss=0.000208, Fidelity=0.9998\n",
      "Early stopping at epoch 132\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9998 ± 0.0001\n",
      "  RMSE (x,y,z): (0.0163, 0.0138, 0.0129)\n",
      "  Frac > 0.95: 1.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.045115, Val Loss=0.001120, Fidelity=0.9908\n",
      "Epoch 50: Train Loss=0.001790, Val Loss=0.000243, Fidelity=0.9998\n",
      "Epoch 100: Train Loss=0.001582, Val Loss=0.000256, Fidelity=0.9998\n",
      "Epoch 150: Train Loss=0.001475, Val Loss=0.000222, Fidelity=0.9998\n",
      "Epoch 200: Train Loss=0.001449, Val Loss=0.000199, Fidelity=0.9999\n",
      "Epoch 250: Train Loss=0.001437, Val Loss=0.000224, Fidelity=0.9998\n",
      "Epoch 300: Train Loss=0.001397, Val Loss=0.000246, Fidelity=0.9998\n",
      "Early stopping at epoch 333\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9998 ± 0.0002\n",
      "  RMSE (x,y,z): (0.0128, 0.0163, 0.0160)\n",
      "  Frac > 0.95: 1.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_XY_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.187613, Val Loss=0.166137, Fidelity=0.7454\n",
      "Epoch 50: Train Loss=0.168290, Val Loss=0.165559, Fidelity=0.7551\n",
      "Epoch 100: Train Loss=0.168187, Val Loss=0.165587, Fidelity=0.7547\n",
      "Epoch 150: Train Loss=0.168199, Val Loss=0.165541, Fidelity=0.7538\n",
      "Epoch 200: Train Loss=0.168008, Val Loss=0.165510, Fidelity=0.7528\n",
      "Early stopping at epoch 227\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7536 ± 0.1803\n",
      "  RMSE (x,y,z): (0.0165, 0.0147, 0.7069)\n",
      "  Frac > 0.95: 0.2431\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_XY_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.188110, Val Loss=0.167190, Fidelity=0.7459\n",
      "Epoch 50: Train Loss=0.168251, Val Loss=0.166676, Fidelity=0.7543\n",
      "Epoch 100: Train Loss=0.168044, Val Loss=0.166651, Fidelity=0.7549\n",
      "Epoch 150: Train Loss=0.168024, Val Loss=0.166642, Fidelity=0.7536\n",
      "Early stopping at epoch 198\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7516 ± 0.1809\n",
      "  RMSE (x,y,z): (0.0148, 0.0158, 0.7078)\n",
      "  Frac > 0.95: 0.2383\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_XY_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.191276, Val Loss=0.169135, Fidelity=0.7414\n",
      "Epoch 50: Train Loss=0.167908, Val Loss=0.168762, Fidelity=0.7517\n",
      "Epoch 100: Train Loss=0.167764, Val Loss=0.168785, Fidelity=0.7510\n",
      "Epoch 150: Train Loss=0.167721, Val Loss=0.168771, Fidelity=0.7480\n",
      "Early stopping at epoch 175\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7474 ± 0.1792\n",
      "  RMSE (x,y,z): (0.0138, 0.0158, 0.7114)\n",
      "  Frac > 0.95: 0.2270\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.116447, Val Loss=0.085045, Fidelity=0.8704\n",
      "Epoch 50: Train Loss=0.084257, Val Loss=0.084228, Fidelity=0.8806\n",
      "Epoch 100: Train Loss=0.084001, Val Loss=0.084165, Fidelity=0.8774\n",
      "Epoch 150: Train Loss=0.084007, Val Loss=0.084180, Fidelity=0.8764\n",
      "Epoch 200: Train Loss=0.083875, Val Loss=0.084177, Fidelity=0.8756\n",
      "Epoch 250: Train Loss=0.083949, Val Loss=0.084174, Fidelity=0.8759\n",
      "Early stopping at epoch 289\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8769 ± 0.1443\n",
      "  RMSE (x,y,z): (0.0165, 0.5018, 0.0191)\n",
      "  Frac > 0.95: 0.5045\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.114537, Val Loss=0.083784, Fidelity=0.8690\n",
      "Epoch 50: Train Loss=0.085035, Val Loss=0.082957, Fidelity=0.8814\n",
      "Epoch 100: Train Loss=0.084858, Val Loss=0.082939, Fidelity=0.8811\n",
      "Epoch 150: Train Loss=0.084902, Val Loss=0.082906, Fidelity=0.8806\n",
      "Epoch 200: Train Loss=0.084808, Val Loss=0.082881, Fidelity=0.8781\n",
      "Epoch 250: Train Loss=0.084771, Val Loss=0.082873, Fidelity=0.8793\n",
      "Epoch 300: Train Loss=0.084850, Val Loss=0.082880, Fidelity=0.8775\n",
      "Early stopping at epoch 304\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8779 ± 0.1448\n",
      "  RMSE (x,y,z): (0.0177, 0.5011, 0.0185)\n",
      "  Frac > 0.95: 0.5105\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.114949, Val Loss=0.083928, Fidelity=0.8711\n",
      "Epoch 50: Train Loss=0.084931, Val Loss=0.082872, Fidelity=0.8799\n",
      "Epoch 100: Train Loss=0.084750, Val Loss=0.082855, Fidelity=0.8781\n",
      "Epoch 150: Train Loss=0.084651, Val Loss=0.082877, Fidelity=0.8794\n",
      "Epoch 200: Train Loss=0.084739, Val Loss=0.082875, Fidelity=0.8799\n",
      "Epoch 250: Train Loss=0.084664, Val Loss=0.082885, Fidelity=0.8794\n",
      "Epoch 300: Train Loss=0.084718, Val Loss=0.082821, Fidelity=0.8786\n",
      "Epoch 350: Train Loss=0.084611, Val Loss=0.082839, Fidelity=0.8786\n",
      "Early stopping at epoch 362\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8812 ± 0.1403\n",
      "  RMSE (x,y,z): (0.0152, 0.4943, 0.0176)\n",
      "  Frac > 0.95: 0.5033\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_YZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.115592, Val Loss=0.085217, Fidelity=0.8706\n",
      "Epoch 50: Train Loss=0.085275, Val Loss=0.084343, Fidelity=0.8801\n",
      "Epoch 100: Train Loss=0.085164, Val Loss=0.084311, Fidelity=0.8795\n",
      "Epoch 150: Train Loss=0.085115, Val Loss=0.084262, Fidelity=0.8788\n",
      "Epoch 200: Train Loss=0.085142, Val Loss=0.084262, Fidelity=0.8800\n",
      "Epoch 250: Train Loss=0.085106, Val Loss=0.084309, Fidelity=0.8795\n",
      "Epoch 300: Train Loss=0.085076, Val Loss=0.084300, Fidelity=0.8797\n",
      "Epoch 350: Train Loss=0.085085, Val Loss=0.084229, Fidelity=0.8773\n",
      "Early stopping at epoch 390\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8811 ± 0.1424\n",
      "  RMSE (x,y,z): (0.4988, 0.0189, 0.0165)\n",
      "  Frac > 0.95: 0.5103\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_YZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.117055, Val Loss=0.085793, Fidelity=0.8706\n",
      "Epoch 50: Train Loss=0.084628, Val Loss=0.084608, Fidelity=0.8802\n",
      "Epoch 100: Train Loss=0.084512, Val Loss=0.084440, Fidelity=0.8778\n",
      "Epoch 150: Train Loss=0.084521, Val Loss=0.084500, Fidelity=0.8778\n",
      "Early stopping at epoch 200\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8805 ± 0.1422\n",
      "  RMSE (x,y,z): (0.4979, 0.0161, 0.0209)\n",
      "  Frac > 0.95: 0.5123\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_pure_YZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.113547, Val Loss=0.083788, Fidelity=0.8684\n",
      "Epoch 50: Train Loss=0.084928, Val Loss=0.082556, Fidelity=0.8811\n",
      "Epoch 100: Train Loss=0.084783, Val Loss=0.082645, Fidelity=0.8833\n",
      "Epoch 150: Train Loss=0.084638, Val Loss=0.082591, Fidelity=0.8809\n",
      "Early stopping at epoch 188\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8796 ± 0.1429\n",
      "  RMSE (x,y,z): (0.4999, 0.0175, 0.0157)\n",
      "  Frac > 0.95: 0.5057\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.043207, Val Loss=0.000865, Fidelity=0.9811\n",
      "Epoch 50: Train Loss=0.002063, Val Loss=0.000335, Fidelity=0.9898\n",
      "Epoch 100: Train Loss=0.001899, Val Loss=0.000345, Fidelity=0.9898\n",
      "Epoch 150: Train Loss=0.001794, Val Loss=0.000367, Fidelity=0.9898\n",
      "Early stopping at epoch 193\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9898 ± 0.0002\n",
      "  RMSE (x,y,z): (0.0207, 0.0172, 0.0192)\n",
      "  Frac > 0.95: 1.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.048099, Val Loss=0.000841, Fidelity=0.9806\n",
      "Epoch 50: Train Loss=0.001936, Val Loss=0.000324, Fidelity=0.9899\n",
      "Epoch 100: Train Loss=0.001773, Val Loss=0.000357, Fidelity=0.9898\n",
      "Epoch 150: Train Loss=0.001670, Val Loss=0.000295, Fidelity=0.9899\n",
      "Epoch 200: Train Loss=0.001653, Val Loss=0.000342, Fidelity=0.9898\n",
      "Epoch 250: Train Loss=0.001632, Val Loss=0.000412, Fidelity=0.9898\n",
      "Early stopping at epoch 251\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9898 ± 0.0002\n",
      "  RMSE (x,y,z): (0.0183, 0.0176, 0.0221)\n",
      "  Frac > 0.95: 1.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.041576, Val Loss=0.000860, Fidelity=0.9818\n",
      "Epoch 50: Train Loss=0.001971, Val Loss=0.000330, Fidelity=0.9899\n",
      "Epoch 100: Train Loss=0.001768, Val Loss=0.000314, Fidelity=0.9899\n",
      "Epoch 150: Train Loss=0.001699, Val Loss=0.000334, Fidelity=0.9898\n",
      "Epoch 200: Train Loss=0.001642, Val Loss=0.000340, Fidelity=0.9898\n",
      "Epoch 250: Train Loss=0.001617, Val Loss=0.000330, Fidelity=0.9899\n",
      "Epoch 300: Train Loss=0.001570, Val Loss=0.000324, Fidelity=0.9899\n",
      "Epoch 350: Train Loss=0.001548, Val Loss=0.000360, Fidelity=0.9898\n",
      "Early stopping at epoch 363\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9898 ± 0.0002\n",
      "  RMSE (x,y,z): (0.0193, 0.0183, 0.0176)\n",
      "  Frac > 0.95: 1.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_XY_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.133065, Val Loss=0.107003, Fidelity=0.8230\n",
      "Epoch 50: Train Loss=0.108220, Val Loss=0.106375, Fidelity=0.8288\n",
      "Epoch 100: Train Loss=0.108056, Val Loss=0.106430, Fidelity=0.8264\n",
      "Epoch 150: Train Loss=0.108048, Val Loss=0.106402, Fidelity=0.8268\n",
      "Early stopping at epoch 181\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8249 ± 0.1495\n",
      "  RMSE (x,y,z): (0.0180, 0.0223, 0.5661)\n",
      "  Frac > 0.95: 0.3151\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_XY_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.131177, Val Loss=0.107318, Fidelity=0.8201\n",
      "Epoch 50: Train Loss=0.108816, Val Loss=0.106650, Fidelity=0.8257\n",
      "Epoch 100: Train Loss=0.108741, Val Loss=0.106732, Fidelity=0.8254\n",
      "Early stopping at epoch 135\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8241 ± 0.1496\n",
      "  RMSE (x,y,z): (0.0193, 0.0163, 0.5670)\n",
      "  Frac > 0.95: 0.3128\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_XY_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.134061, Val Loss=0.107306, Fidelity=0.8135\n",
      "Epoch 50: Train Loss=0.108091, Val Loss=0.106665, Fidelity=0.8286\n",
      "Epoch 100: Train Loss=0.107898, Val Loss=0.106701, Fidelity=0.8255\n",
      "Epoch 150: Train Loss=0.107867, Val Loss=0.106648, Fidelity=0.8267\n",
      "Epoch 200: Train Loss=0.107888, Val Loss=0.106617, Fidelity=0.8262\n",
      "Epoch 250: Train Loss=0.107828, Val Loss=0.106604, Fidelity=0.8256\n",
      "Early stopping at epoch 257\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8261 ± 0.1471\n",
      "  RMSE (x,y,z): (0.0171, 0.0172, 0.5640)\n",
      "  Frac > 0.95: 0.3077\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.131959, Val Loss=0.108442, Fidelity=0.8189\n",
      "Epoch 50: Train Loss=0.108625, Val Loss=0.107908, Fidelity=0.8242\n",
      "Epoch 100: Train Loss=0.108444, Val Loss=0.107896, Fidelity=0.8241\n",
      "Epoch 150: Train Loss=0.108411, Val Loss=0.107857, Fidelity=0.8247\n",
      "Epoch 200: Train Loss=0.108337, Val Loss=0.107880, Fidelity=0.8230\n",
      "Early stopping at epoch 221\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8240 ± 0.1493\n",
      "  RMSE (x,y,z): (0.0197, 0.5667, 0.0170)\n",
      "  Frac > 0.95: 0.3109\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.131983, Val Loss=0.106852, Fidelity=0.8187\n",
      "Epoch 50: Train Loss=0.108517, Val Loss=0.105819, Fidelity=0.8293\n",
      "Epoch 100: Train Loss=0.108327, Val Loss=0.105772, Fidelity=0.8286\n",
      "Epoch 150: Train Loss=0.108265, Val Loss=0.105780, Fidelity=0.8261\n",
      "Epoch 200: Train Loss=0.108168, Val Loss=0.105799, Fidelity=0.8288\n",
      "Epoch 250: Train Loss=0.108200, Val Loss=0.105774, Fidelity=0.8268\n",
      "Epoch 300: Train Loss=0.108207, Val Loss=0.105748, Fidelity=0.8271\n",
      "Epoch 350: Train Loss=0.108196, Val Loss=0.105793, Fidelity=0.8270\n",
      "Epoch 400: Train Loss=0.108231, Val Loss=0.105826, Fidelity=0.8266\n",
      "Epoch 450: Train Loss=0.108207, Val Loss=0.105775, Fidelity=0.8267\n",
      "Epoch 500: Train Loss=0.108089, Val Loss=0.105736, Fidelity=0.8271\n",
      "Early stopping at epoch 538\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8263 ± 0.1465\n",
      "  RMSE (x,y,z): (0.0180, 0.5620, 0.0175)\n",
      "  Frac > 0.95: 0.3023\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.133165, Val Loss=0.108118, Fidelity=0.8131\n",
      "Epoch 50: Train Loss=0.108055, Val Loss=0.107548, Fidelity=0.8229\n",
      "Epoch 100: Train Loss=0.108061, Val Loss=0.107559, Fidelity=0.8246\n",
      "Epoch 150: Train Loss=0.107968, Val Loss=0.107522, Fidelity=0.8260\n",
      "Epoch 200: Train Loss=0.107955, Val Loss=0.107513, Fidelity=0.8238\n",
      "Early stopping at epoch 203\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8216 ± 0.1485\n",
      "  RMSE (x,y,z): (0.0169, 0.5706, 0.0205)\n",
      "  Frac > 0.95: 0.2975\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_YZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.131634, Val Loss=0.107375, Fidelity=0.8174\n",
      "Epoch 50: Train Loss=0.108420, Val Loss=0.106545, Fidelity=0.8260\n",
      "Epoch 100: Train Loss=0.108288, Val Loss=0.106553, Fidelity=0.8254\n",
      "Epoch 150: Train Loss=0.108250, Val Loss=0.106537, Fidelity=0.8240\n",
      "Epoch 200: Train Loss=0.108320, Val Loss=0.106547, Fidelity=0.8248\n",
      "Epoch 250: Train Loss=0.108177, Val Loss=0.106544, Fidelity=0.8255\n",
      "Early stopping at epoch 256\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8254 ± 0.1476\n",
      "  RMSE (x,y,z): (0.5647, 0.0180, 0.0187)\n",
      "  Frac > 0.95: 0.3055\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_YZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.134940, Val Loss=0.109192, Fidelity=0.8145\n",
      "Epoch 50: Train Loss=0.107973, Val Loss=0.108441, Fidelity=0.8246\n",
      "Epoch 100: Train Loss=0.107947, Val Loss=0.108410, Fidelity=0.8230\n",
      "Epoch 150: Train Loss=0.107823, Val Loss=0.108398, Fidelity=0.8233\n",
      "Epoch 200: Train Loss=0.107886, Val Loss=0.108438, Fidelity=0.8238\n",
      "Early stopping at epoch 220\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8243 ± 0.1492\n",
      "  RMSE (x,y,z): (0.5684, 0.0183, 0.0184)\n",
      "  Frac > 0.95: 0.3111\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_near_pure_YZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.129676, Val Loss=0.107401, Fidelity=0.8173\n",
      "Epoch 50: Train Loss=0.108946, Val Loss=0.106741, Fidelity=0.8270\n",
      "Epoch 100: Train Loss=0.108869, Val Loss=0.106713, Fidelity=0.8274\n",
      "Epoch 150: Train Loss=0.108792, Val Loss=0.106684, Fidelity=0.8253\n",
      "Epoch 200: Train Loss=0.108803, Val Loss=0.106656, Fidelity=0.8244\n",
      "Epoch 250: Train Loss=0.108708, Val Loss=0.106650, Fidelity=0.8247\n",
      "Epoch 300: Train Loss=0.108751, Val Loss=0.106666, Fidelity=0.8242\n",
      "Early stopping at epoch 319\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8256 ± 0.1467\n",
      "  RMSE (x,y,z): (0.5630, 0.0185, 0.0156)\n",
      "  Frac > 0.95: 0.2944\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.038183, Val Loss=0.001049, Fidelity=0.9096\n",
      "Epoch 50: Train Loss=0.003783, Val Loss=0.000283, Fidelity=0.9105\n",
      "Epoch 100: Train Loss=0.003449, Val Loss=0.000277, Fidelity=0.9089\n",
      "Epoch 150: Train Loss=0.003437, Val Loss=0.000240, Fidelity=0.9112\n",
      "Epoch 200: Train Loss=0.003392, Val Loss=0.000207, Fidelity=0.9109\n",
      "Epoch 250: Train Loss=0.003341, Val Loss=0.000217, Fidelity=0.9091\n",
      "Epoch 300: Train Loss=0.003374, Val Loss=0.000187, Fidelity=0.9091\n",
      "Epoch 350: Train Loss=0.003357, Val Loss=0.000193, Fidelity=0.9094\n",
      "Early stopping at epoch 357\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9100 ± 0.0053\n",
      "  RMSE (x,y,z): (0.0131, 0.0130, 0.0171)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.035341, Val Loss=0.000967, Fidelity=0.9089\n",
      "Epoch 50: Train Loss=0.003685, Val Loss=0.000318, Fidelity=0.9110\n",
      "Epoch 100: Train Loss=0.003420, Val Loss=0.000198, Fidelity=0.9088\n",
      "Epoch 150: Train Loss=0.003351, Val Loss=0.000202, Fidelity=0.9078\n",
      "Early stopping at epoch 164\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9105 ± 0.0041\n",
      "  RMSE (x,y,z): (0.0115, 0.0136, 0.0163)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.039199, Val Loss=0.000908, Fidelity=0.9071\n",
      "Epoch 50: Train Loss=0.003643, Val Loss=0.000233, Fidelity=0.9114\n",
      "Epoch 100: Train Loss=0.003439, Val Loss=0.000172, Fidelity=0.9094\n",
      "Epoch 150: Train Loss=0.003398, Val Loss=0.000197, Fidelity=0.9110\n",
      "Epoch 200: Train Loss=0.003332, Val Loss=0.000190, Fidelity=0.9085\n",
      "Epoch 250: Train Loss=0.003314, Val Loss=0.000179, Fidelity=0.9104\n",
      "Epoch 300: Train Loss=0.003277, Val Loss=0.000199, Fidelity=0.9103\n",
      "Epoch 350: Train Loss=0.003294, Val Loss=0.000177, Fidelity=0.9079\n",
      "Early stopping at epoch 352\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9086 ± 0.0047\n",
      "  RMSE (x,y,z): (0.0128, 0.0118, 0.0157)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_XY_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.154100, Val Loss=0.134518, Fidelity=0.7008\n",
      "Epoch 50: Train Loss=0.136761, Val Loss=0.134001, Fidelity=0.7053\n",
      "Epoch 100: Train Loss=0.136638, Val Loss=0.134097, Fidelity=0.7060\n",
      "Early stopping at epoch 140\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7035 ± 0.1437\n",
      "  RMSE (x,y,z): (0.0113, 0.0108, 0.6361)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_XY_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.153147, Val Loss=0.135165, Fidelity=0.6975\n",
      "Epoch 50: Train Loss=0.136471, Val Loss=0.134972, Fidelity=0.7054\n",
      "Epoch 100: Train Loss=0.136523, Val Loss=0.134933, Fidelity=0.7041\n",
      "Epoch 150: Train Loss=0.136409, Val Loss=0.134909, Fidelity=0.7035\n",
      "Epoch 200: Train Loss=0.136450, Val Loss=0.134925, Fidelity=0.7041\n",
      "Epoch 250: Train Loss=0.136421, Val Loss=0.134948, Fidelity=0.7033\n",
      "Epoch 300: Train Loss=0.136444, Val Loss=0.134947, Fidelity=0.7035\n",
      "Epoch 350: Train Loss=0.136368, Val Loss=0.134898, Fidelity=0.7035\n",
      "Epoch 400: Train Loss=0.136415, Val Loss=0.134911, Fidelity=0.7026\n",
      "Early stopping at epoch 438\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7030 ± 0.1451\n",
      "  RMSE (x,y,z): (0.0099, 0.0127, 0.6371)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_XY_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.153567, Val Loss=0.137511, Fidelity=0.6968\n",
      "Epoch 50: Train Loss=0.136386, Val Loss=0.136658, Fidelity=0.7007\n",
      "Epoch 100: Train Loss=0.136098, Val Loss=0.136624, Fidelity=0.7018\n",
      "Epoch 150: Train Loss=0.136212, Val Loss=0.136647, Fidelity=0.7013\n",
      "Early stopping at epoch 171\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7003 ± 0.1440\n",
      "  RMSE (x,y,z): (0.0144, 0.0112, 0.6404)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.094609, Val Loss=0.068465, Fidelity=0.7992\n",
      "Epoch 50: Train Loss=0.068696, Val Loss=0.068098, Fidelity=0.8043\n",
      "Epoch 100: Train Loss=0.068563, Val Loss=0.068116, Fidelity=0.8058\n",
      "Epoch 150: Train Loss=0.068562, Val Loss=0.068106, Fidelity=0.8048\n",
      "Epoch 200: Train Loss=0.068515, Val Loss=0.068110, Fidelity=0.8049\n",
      "Epoch 250: Train Loss=0.068563, Val Loss=0.068083, Fidelity=0.8042\n",
      "Early stopping at epoch 298\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8052 ± 0.1159\n",
      "  RMSE (x,y,z): (0.0109, 0.4517, 0.0118)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.090637, Val Loss=0.067404, Fidelity=0.8030\n",
      "Epoch 50: Train Loss=0.069469, Val Loss=0.067116, Fidelity=0.8064\n",
      "Epoch 100: Train Loss=0.069408, Val Loss=0.067086, Fidelity=0.8076\n",
      "Epoch 150: Train Loss=0.069433, Val Loss=0.067102, Fidelity=0.8073\n",
      "Epoch 200: Train Loss=0.069360, Val Loss=0.067139, Fidelity=0.8075\n",
      "Epoch 250: Train Loss=0.069267, Val Loss=0.067102, Fidelity=0.8064\n",
      "Early stopping at epoch 284\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8039 ± 0.1153\n",
      "  RMSE (x,y,z): (0.0134, 0.4511, 0.0098)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.092658, Val Loss=0.067396, Fidelity=0.7991\n",
      "Epoch 50: Train Loss=0.069437, Val Loss=0.067109, Fidelity=0.8047\n",
      "Epoch 100: Train Loss=0.069262, Val Loss=0.067031, Fidelity=0.8053\n",
      "Epoch 150: Train Loss=0.069233, Val Loss=0.067054, Fidelity=0.8072\n",
      "Epoch 200: Train Loss=0.069180, Val Loss=0.067068, Fidelity=0.8074\n",
      "Early stopping at epoch 211\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8088 ± 0.1132\n",
      "  RMSE (x,y,z): (0.0101, 0.4449, 0.0162)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_YZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.094329, Val Loss=0.068694, Fidelity=0.7997\n",
      "Epoch 50: Train Loss=0.069734, Val Loss=0.068232, Fidelity=0.8060\n",
      "Epoch 100: Train Loss=0.069574, Val Loss=0.068214, Fidelity=0.8059\n",
      "Epoch 150: Train Loss=0.069547, Val Loss=0.068195, Fidelity=0.8064\n",
      "Early stopping at epoch 159\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8046 ± 0.1141\n",
      "  RMSE (x,y,z): (0.4489, 0.0098, 0.0117)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_YZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.094235, Val Loss=0.068812, Fidelity=0.8012\n",
      "Epoch 50: Train Loss=0.069042, Val Loss=0.068441, Fidelity=0.8068\n",
      "Epoch 100: Train Loss=0.069020, Val Loss=0.068375, Fidelity=0.8046\n",
      "Epoch 150: Train Loss=0.068978, Val Loss=0.068367, Fidelity=0.8042\n",
      "Epoch 200: Train Loss=0.068957, Val Loss=0.068434, Fidelity=0.8064\n",
      "Epoch 250: Train Loss=0.068932, Val Loss=0.068351, Fidelity=0.8057\n",
      "Early stopping at epoch 266\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8077 ± 0.1137\n",
      "  RMSE (x,y,z): (0.4481, 0.0112, 0.0135)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.1_YZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.091630, Val Loss=0.067130, Fidelity=0.8011\n",
      "Epoch 50: Train Loss=0.069380, Val Loss=0.066848, Fidelity=0.8057\n",
      "Epoch 100: Train Loss=0.069227, Val Loss=0.066885, Fidelity=0.8051\n",
      "Early stopping at epoch 122\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8071 ± 0.1152\n",
      "  RMSE (x,y,z): (0.4499, 0.0103, 0.0134)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.027551, Val Loss=0.000353, Fidelity=0.7781\n",
      "Epoch 50: Train Loss=0.003244, Val Loss=0.000129, Fidelity=0.7805\n",
      "Epoch 100: Train Loss=0.003097, Val Loss=0.000122, Fidelity=0.7804\n",
      "Epoch 150: Train Loss=0.003015, Val Loss=0.000112, Fidelity=0.7810\n",
      "Epoch 200: Train Loss=0.003029, Val Loss=0.000097, Fidelity=0.7813\n",
      "Epoch 250: Train Loss=0.003011, Val Loss=0.000127, Fidelity=0.7822\n",
      "Epoch 300: Train Loss=0.002967, Val Loss=0.000110, Fidelity=0.7816\n",
      "Epoch 350: Train Loss=0.002972, Val Loss=0.000093, Fidelity=0.7812\n",
      "Epoch 400: Train Loss=0.002979, Val Loss=0.000102, Fidelity=0.7816\n",
      "Epoch 450: Train Loss=0.002949, Val Loss=0.000091, Fidelity=0.7822\n",
      "Epoch 500: Train Loss=0.002967, Val Loss=0.000096, Fidelity=0.7810\n",
      "Epoch 550: Train Loss=0.002968, Val Loss=0.000095, Fidelity=0.7822\n",
      "Early stopping at epoch 552\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7812 ± 0.0023\n",
      "  RMSE (x,y,z): (0.0095, 0.0094, 0.0074)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.026354, Val Loss=0.000543, Fidelity=0.7729\n",
      "Epoch 50: Train Loss=0.003223, Val Loss=0.000113, Fidelity=0.7822\n",
      "Epoch 100: Train Loss=0.003083, Val Loss=0.000153, Fidelity=0.7817\n",
      "Early stopping at epoch 119\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7812 ± 0.0029\n",
      "  RMSE (x,y,z): (0.0118, 0.0115, 0.0114)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.027730, Val Loss=0.000455, Fidelity=0.7758\n",
      "Epoch 50: Train Loss=0.003323, Val Loss=0.000126, Fidelity=0.7792\n",
      "Epoch 100: Train Loss=0.003203, Val Loss=0.000134, Fidelity=0.7807\n",
      "Epoch 150: Train Loss=0.003145, Val Loss=0.000136, Fidelity=0.7794\n",
      "Epoch 200: Train Loss=0.003099, Val Loss=0.000110, Fidelity=0.7803\n",
      "Epoch 250: Train Loss=0.003031, Val Loss=0.000117, Fidelity=0.7830\n",
      "Epoch 300: Train Loss=0.003021, Val Loss=0.000104, Fidelity=0.7825\n",
      "Early stopping at epoch 323\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7830 ± 0.0032\n",
      "  RMSE (x,y,z): (0.0098, 0.0118, 0.0138)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_XY_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.106995, Val Loss=0.093346, Fidelity=0.6373\n",
      "Epoch 50: Train Loss=0.095165, Val Loss=0.093051, Fidelity=0.6407\n",
      "Epoch 100: Train Loss=0.095112, Val Loss=0.093038, Fidelity=0.6414\n",
      "Epoch 150: Train Loss=0.095059, Val Loss=0.093088, Fidelity=0.6405\n",
      "Epoch 200: Train Loss=0.095084, Val Loss=0.093051, Fidelity=0.6415\n",
      "Early stopping at epoch 209\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.6407 ± 0.0998\n",
      "  RMSE (x,y,z): (0.0099, 0.0090, 0.5302)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_XY_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.106452, Val Loss=0.093790, Fidelity=0.6377\n",
      "Epoch 50: Train Loss=0.095064, Val Loss=0.093673, Fidelity=0.6412\n",
      "Epoch 100: Train Loss=0.094976, Val Loss=0.093733, Fidelity=0.6376\n",
      "Early stopping at epoch 115\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.6390 ± 0.0988\n",
      "  RMSE (x,y,z): (0.0109, 0.0091, 0.5309)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_XY_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.105476, Val Loss=0.095089, Fidelity=0.6376\n",
      "Epoch 50: Train Loss=0.094959, Val Loss=0.094864, Fidelity=0.6391\n",
      "Epoch 100: Train Loss=0.094852, Val Loss=0.094859, Fidelity=0.6388\n",
      "Early stopping at epoch 114\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.6388 ± 0.0988\n",
      "  RMSE (x,y,z): (0.0097, 0.0082, 0.5337)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.065219, Val Loss=0.047555, Fidelity=0.7065\n",
      "Epoch 50: Train Loss=0.048180, Val Loss=0.047300, Fidelity=0.7116\n",
      "Epoch 100: Train Loss=0.048061, Val Loss=0.047308, Fidelity=0.7085\n",
      "Epoch 150: Train Loss=0.048029, Val Loss=0.047293, Fidelity=0.7107\n",
      "Epoch 200: Train Loss=0.048018, Val Loss=0.047298, Fidelity=0.7100\n",
      "Epoch 250: Train Loss=0.047961, Val Loss=0.047280, Fidelity=0.7113\n",
      "Epoch 300: Train Loss=0.048019, Val Loss=0.047275, Fidelity=0.7098\n",
      "Epoch 350: Train Loss=0.048033, Val Loss=0.047305, Fidelity=0.7115\n",
      "Early stopping at epoch 388\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7106 ± 0.0790\n",
      "  RMSE (x,y,z): (0.0096, 0.3764, 0.0091)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.066133, Val Loss=0.046801, Fidelity=0.7066\n",
      "Epoch 50: Train Loss=0.048631, Val Loss=0.046583, Fidelity=0.7117\n",
      "Epoch 100: Train Loss=0.048551, Val Loss=0.046560, Fidelity=0.7113\n",
      "Early stopping at epoch 112\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7122 ± 0.0800\n",
      "  RMSE (x,y,z): (0.0095, 0.3758, 0.0141)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.062871, Val Loss=0.046739, Fidelity=0.7083\n",
      "Epoch 50: Train Loss=0.048521, Val Loss=0.046639, Fidelity=0.7108\n",
      "Epoch 100: Train Loss=0.048451, Val Loss=0.046582, Fidelity=0.7116\n",
      "Epoch 150: Train Loss=0.048397, Val Loss=0.046556, Fidelity=0.7117\n",
      "Early stopping at epoch 199\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7134 ± 0.0778\n",
      "  RMSE (x,y,z): (0.0091, 0.3707, 0.0089)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_YZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.066578, Val Loss=0.047712, Fidelity=0.7057\n",
      "Epoch 50: Train Loss=0.048816, Val Loss=0.047374, Fidelity=0.7095\n",
      "Epoch 100: Train Loss=0.048726, Val Loss=0.047364, Fidelity=0.7101\n",
      "Epoch 150: Train Loss=0.048709, Val Loss=0.047387, Fidelity=0.7104\n",
      "Epoch 200: Train Loss=0.048748, Val Loss=0.047361, Fidelity=0.7108\n",
      "Epoch 250: Train Loss=0.048710, Val Loss=0.047338, Fidelity=0.7103\n",
      "Epoch 300: Train Loss=0.048687, Val Loss=0.047381, Fidelity=0.7111\n",
      "Epoch 350: Train Loss=0.048700, Val Loss=0.047350, Fidelity=0.7106\n",
      "Early stopping at epoch 352\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7113 ± 0.0789\n",
      "  RMSE (x,y,z): (0.3740, 0.0088, 0.0091)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_YZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.064004, Val Loss=0.047901, Fidelity=0.7092\n",
      "Epoch 50: Train Loss=0.048392, Val Loss=0.047458, Fidelity=0.7089\n",
      "Epoch 100: Train Loss=0.048291, Val Loss=0.047478, Fidelity=0.7111\n",
      "Epoch 150: Train Loss=0.048286, Val Loss=0.047483, Fidelity=0.7128\n",
      "Epoch 200: Train Loss=0.048206, Val Loss=0.047499, Fidelity=0.7096\n",
      "Epoch 250: Train Loss=0.048195, Val Loss=0.047478, Fidelity=0.7117\n",
      "Epoch 300: Train Loss=0.048221, Val Loss=0.047476, Fidelity=0.7124\n",
      "Early stopping at epoch 303\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7114 ± 0.0789\n",
      "  RMSE (x,y,z): (0.3735, 0.0089, 0.0096)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.25_YZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.063042, Val Loss=0.046554, Fidelity=0.7105\n",
      "Epoch 50: Train Loss=0.048634, Val Loss=0.046407, Fidelity=0.7117\n",
      "Epoch 100: Train Loss=0.048559, Val Loss=0.046432, Fidelity=0.7123\n",
      "Epoch 150: Train Loss=0.048488, Val Loss=0.046404, Fidelity=0.7122\n",
      "Epoch 200: Train Loss=0.048489, Val Loss=0.046397, Fidelity=0.7128\n",
      "Epoch 250: Train Loss=0.048476, Val Loss=0.046424, Fidelity=0.7125\n",
      "Early stopping at epoch 299\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.7089 ± 0.0777\n",
      "  RMSE (x,y,z): (0.3750, 0.0093, 0.0128)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.012808, Val Loss=0.000223, Fidelity=0.6207\n",
      "Epoch 50: Train Loss=0.001934, Val Loss=0.000089, Fidelity=0.6230\n",
      "Epoch 100: Train Loss=0.001871, Val Loss=0.000079, Fidelity=0.6231\n",
      "Epoch 150: Train Loss=0.001859, Val Loss=0.000096, Fidelity=0.6226\n",
      "Epoch 200: Train Loss=0.001811, Val Loss=0.000081, Fidelity=0.6228\n",
      "Epoch 250: Train Loss=0.001822, Val Loss=0.000090, Fidelity=0.6226\n",
      "Early stopping at epoch 255\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.6230 ± 0.0012\n",
      "  RMSE (x,y,z): (0.0086, 0.0095, 0.0095)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.012728, Val Loss=0.000283, Fidelity=0.6206\n",
      "Epoch 50: Train Loss=0.001855, Val Loss=0.000081, Fidelity=0.6236\n",
      "Epoch 100: Train Loss=0.001773, Val Loss=0.000087, Fidelity=0.6230\n",
      "Epoch 150: Train Loss=0.001739, Val Loss=0.000061, Fidelity=0.6238\n",
      "Epoch 200: Train Loss=0.001716, Val Loss=0.000060, Fidelity=0.6235\n",
      "Epoch 250: Train Loss=0.001734, Val Loss=0.000090, Fidelity=0.6227\n",
      "Early stopping at epoch 266\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.6227 ± 0.0010\n",
      "  RMSE (x,y,z): (0.0085, 0.0081, 0.0103)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.014110, Val Loss=0.000308, Fidelity=0.6204\n",
      "Epoch 50: Train Loss=0.001863, Val Loss=0.000063, Fidelity=0.6233\n",
      "Epoch 100: Train Loss=0.001776, Val Loss=0.000109, Fidelity=0.6223\n",
      "Epoch 150: Train Loss=0.001751, Val Loss=0.000065, Fidelity=0.6233\n",
      "Epoch 200: Train Loss=0.001727, Val Loss=0.000071, Fidelity=0.6229\n",
      "Epoch 250: Train Loss=0.001717, Val Loss=0.000067, Fidelity=0.6236\n",
      "Early stopping at epoch 251\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.6236 ± 0.0014\n",
      "  RMSE (x,y,z): (0.0087, 0.0088, 0.0075)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_XY_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.051633, Val Loss=0.041526, Fidelity=0.5601\n",
      "Epoch 50: Train Loss=0.042520, Val Loss=0.041423, Fidelity=0.5607\n",
      "Epoch 100: Train Loss=0.042456, Val Loss=0.041414, Fidelity=0.5611\n",
      "Epoch 150: Train Loss=0.042457, Val Loss=0.041395, Fidelity=0.5616\n",
      "Early stopping at epoch 152\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.5608 ± 0.0428\n",
      "  RMSE (x,y,z): (0.0105, 0.0086, 0.3534)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_XY_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.048443, Val Loss=0.041813, Fidelity=0.5599\n",
      "Epoch 50: Train Loss=0.042451, Val Loss=0.041670, Fidelity=0.5622\n",
      "Epoch 100: Train Loss=0.042398, Val Loss=0.041673, Fidelity=0.5617\n",
      "Early stopping at epoch 135\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.5605 ± 0.0427\n",
      "  RMSE (x,y,z): (0.0109, 0.0094, 0.3540)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_XY_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.047089, Val Loss=0.042324, Fidelity=0.5597\n",
      "Epoch 50: Train Loss=0.042370, Val Loss=0.042195, Fidelity=0.5602\n",
      "Epoch 100: Train Loss=0.042359, Val Loss=0.042182, Fidelity=0.5609\n",
      "Early stopping at epoch 107\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.5609 ± 0.0432\n",
      "  RMSE (x,y,z): (0.0080, 0.0074, 0.3557)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.029185, Val Loss=0.021200, Fidelity=0.5906\n",
      "Epoch 50: Train Loss=0.021650, Val Loss=0.021043, Fidelity=0.5922\n",
      "Epoch 100: Train Loss=0.021620, Val Loss=0.021022, Fidelity=0.5928\n",
      "Epoch 150: Train Loss=0.021585, Val Loss=0.021026, Fidelity=0.5925\n",
      "Epoch 200: Train Loss=0.021572, Val Loss=0.021027, Fidelity=0.5924\n",
      "Epoch 250: Train Loss=0.021611, Val Loss=0.021054, Fidelity=0.5931\n",
      "Early stopping at epoch 273\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.5924 ± 0.0346\n",
      "  RMSE (x,y,z): (0.0106, 0.2509, 0.0075)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.029258, Val Loss=0.020869, Fidelity=0.5919\n",
      "Epoch 50: Train Loss=0.021872, Val Loss=0.020713, Fidelity=0.5936\n",
      "Epoch 100: Train Loss=0.021805, Val Loss=0.020713, Fidelity=0.5931\n",
      "Epoch 150: Train Loss=0.021785, Val Loss=0.020705, Fidelity=0.5930\n",
      "Epoch 200: Train Loss=0.021786, Val Loss=0.020731, Fidelity=0.5930\n",
      "Epoch 250: Train Loss=0.021793, Val Loss=0.020736, Fidelity=0.5926\n",
      "Epoch 300: Train Loss=0.021779, Val Loss=0.020726, Fidelity=0.5925\n",
      "Epoch 350: Train Loss=0.021788, Val Loss=0.020715, Fidelity=0.5936\n",
      "Early stopping at epoch 380\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.5931 ± 0.0351\n",
      "  RMSE (x,y,z): (0.0064, 0.2506, 0.0069)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.029238, Val Loss=0.020776, Fidelity=0.5922\n",
      "Epoch 50: Train Loss=0.021826, Val Loss=0.020726, Fidelity=0.5920\n",
      "Epoch 100: Train Loss=0.021787, Val Loss=0.020697, Fidelity=0.5931\n",
      "Epoch 150: Train Loss=0.021773, Val Loss=0.020720, Fidelity=0.5924\n",
      "Epoch 200: Train Loss=0.021774, Val Loss=0.020706, Fidelity=0.5934\n",
      "Early stopping at epoch 216\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.5935 ± 0.0340\n",
      "  RMSE (x,y,z): (0.0077, 0.2471, 0.0074)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_YZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.028349, Val Loss=0.021312, Fidelity=0.5900\n",
      "Epoch 50: Train Loss=0.021896, Val Loss=0.021111, Fidelity=0.5921\n",
      "Epoch 100: Train Loss=0.021861, Val Loss=0.021057, Fidelity=0.5937\n",
      "Epoch 150: Train Loss=0.021844, Val Loss=0.021052, Fidelity=0.5924\n",
      "Epoch 200: Train Loss=0.021860, Val Loss=0.021050, Fidelity=0.5932\n",
      "Epoch 250: Train Loss=0.021833, Val Loss=0.021077, Fidelity=0.5917\n",
      "Early stopping at epoch 267\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.5934 ± 0.0346\n",
      "  RMSE (x,y,z): (0.2494, 0.0060, 0.0068)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_YZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.029144, Val Loss=0.021322, Fidelity=0.5900\n",
      "Epoch 50: Train Loss=0.021762, Val Loss=0.021137, Fidelity=0.5928\n",
      "Epoch 100: Train Loss=0.021728, Val Loss=0.021109, Fidelity=0.5929\n",
      "Epoch 150: Train Loss=0.021703, Val Loss=0.021096, Fidelity=0.5923\n",
      "Epoch 200: Train Loss=0.021682, Val Loss=0.021102, Fidelity=0.5926\n",
      "Epoch 250: Train Loss=0.021694, Val Loss=0.021122, Fidelity=0.5921\n",
      "Epoch 300: Train Loss=0.021668, Val Loss=0.021124, Fidelity=0.5925\n",
      "Early stopping at epoch 333\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.5928 ± 0.0342\n",
      "  RMSE (x,y,z): (0.2490, 0.0076, 0.0107)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "============================================================\n",
      "Running: PriorityB_mixed_p0.5_YZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.028959, Val Loss=0.020678, Fidelity=0.5927\n",
      "Epoch 50: Train Loss=0.021812, Val Loss=0.020635, Fidelity=0.5935\n",
      "Epoch 100: Train Loss=0.021781, Val Loss=0.020635, Fidelity=0.5935\n",
      "Early stopping at epoch 127\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.5931 ± 0.0346\n",
      "  RMSE (x,y,z): (0.2500, 0.0078, 0.0073)\n",
      "  Frac > 0.95: 0.0000\n",
      "\n",
      "Priority B completed: 60 experiments\n",
      "\n",
      "Starting Priority C experiments...\n",
      "\n",
      "================================================================================\n",
      "PRIORITY C: SIC-POVM Input Test\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Running: PriorityC_sic_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.071758, Val Loss=0.004246, Fidelity=0.9065\n",
      "Epoch 50: Train Loss=0.002874, Val Loss=0.000428, Fidelity=0.9272\n",
      "Epoch 100: Train Loss=0.002446, Val Loss=0.000291, Fidelity=0.9257\n",
      "Epoch 150: Train Loss=0.002357, Val Loss=0.000409, Fidelity=0.9248\n",
      "Early stopping at epoch 200\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9238 ± 0.1256\n",
      "  RMSE (x,y,z): (0.0212, 0.0233, 0.0190)\n",
      "  Frac > 0.95: 0.7001\n",
      "\n",
      "============================================================\n",
      "Running: PriorityC_sic_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.077760, Val Loss=0.004102, Fidelity=0.9071\n",
      "Epoch 50: Train Loss=0.002787, Val Loss=0.000409, Fidelity=0.9253\n",
      "Epoch 100: Train Loss=0.002347, Val Loss=0.000285, Fidelity=0.9246\n",
      "Epoch 150: Train Loss=0.002194, Val Loss=0.000286, Fidelity=0.9242\n",
      "Epoch 200: Train Loss=0.002152, Val Loss=0.000287, Fidelity=0.9231\n",
      "Early stopping at epoch 205\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9252 ± 0.1237\n",
      "  RMSE (x,y,z): (0.0160, 0.0182, 0.0175)\n",
      "  Frac > 0.95: 0.7015\n",
      "\n",
      "============================================================\n",
      "Running: PriorityC_sic_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.076125, Val Loss=0.003912, Fidelity=0.9059\n",
      "Epoch 50: Train Loss=0.002869, Val Loss=0.000386, Fidelity=0.9244\n",
      "Epoch 100: Train Loss=0.002411, Val Loss=0.000488, Fidelity=0.9219\n",
      "Epoch 150: Train Loss=0.002314, Val Loss=0.000385, Fidelity=0.9208\n",
      "Epoch 200: Train Loss=0.002214, Val Loss=0.000322, Fidelity=0.9208\n",
      "Early stopping at epoch 227\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9229 ± 0.1266\n",
      "  RMSE (x,y,z): (0.0178, 0.0194, 0.0169)\n",
      "  Frac > 0.95: 0.7009\n",
      "\n",
      "============================================================\n",
      "Running: PriorityC_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.098758, Val Loss=0.070184, Fidelity=0.8156\n",
      "Epoch 50: Train Loss=0.072067, Val Loss=0.069400, Fidelity=0.8272\n",
      "Epoch 100: Train Loss=0.071978, Val Loss=0.069340, Fidelity=0.8254\n",
      "Epoch 150: Train Loss=0.071870, Val Loss=0.069322, Fidelity=0.8232\n",
      "Epoch 200: Train Loss=0.071818, Val Loss=0.069324, Fidelity=0.8259\n",
      "Epoch 250: Train Loss=0.071797, Val Loss=0.069353, Fidelity=0.8245\n",
      "Epoch 300: Train Loss=0.071802, Val Loss=0.069344, Fidelity=0.8242\n",
      "Epoch 350: Train Loss=0.071822, Val Loss=0.069343, Fidelity=0.8238\n",
      "Early stopping at epoch 354\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8198 ± 0.1565\n",
      "  RMSE (x,y,z): (0.0157, 0.4638, 0.0155)\n",
      "  Frac > 0.95: 0.3413\n",
      "\n",
      "============================================================\n",
      "Running: PriorityC_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.099273, Val Loss=0.071684, Fidelity=0.8145\n",
      "Epoch 50: Train Loss=0.072998, Val Loss=0.071008, Fidelity=0.8207\n",
      "Epoch 100: Train Loss=0.072888, Val Loss=0.071021, Fidelity=0.8194\n",
      "Epoch 150: Train Loss=0.072809, Val Loss=0.070988, Fidelity=0.8195\n",
      "Epoch 200: Train Loss=0.072760, Val Loss=0.071026, Fidelity=0.8172\n",
      "Early stopping at epoch 223\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8199 ± 0.1579\n",
      "  RMSE (x,y,z): (0.0137, 0.4583, 0.0180)\n",
      "  Frac > 0.95: 0.3475\n",
      "\n",
      "============================================================\n",
      "Running: PriorityC_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.095893, Val Loss=0.072019, Fidelity=0.8111\n",
      "Epoch 50: Train Loss=0.072387, Val Loss=0.071343, Fidelity=0.8196\n",
      "Epoch 100: Train Loss=0.072356, Val Loss=0.071355, Fidelity=0.8184\n",
      "Epoch 150: Train Loss=0.072309, Val Loss=0.071314, Fidelity=0.8208\n",
      "Early stopping at epoch 174\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8253 ± 0.1557\n",
      "  RMSE (x,y,z): (0.0136, 0.4559, 0.0159)\n",
      "  Frac > 0.95: 0.3539\n",
      "\n",
      "Priority C completed: 6 experiments\n",
      "\n",
      "Starting Priority D experiments...\n",
      "\n",
      "================================================================================\n",
      "PRIORITY D: Adaptive Measurement Protocol\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Running: PriorityD_adaptive_baseline_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.037787, Val Loss=0.001230, Fidelity=0.9199\n",
      "Epoch 50: Train Loss=0.002409, Val Loss=0.000247, Fidelity=0.9280\n",
      "Epoch 100: Train Loss=0.002146, Val Loss=0.000269, Fidelity=0.9272\n",
      "Epoch 150: Train Loss=0.002032, Val Loss=0.000229, Fidelity=0.9272\n",
      "Epoch 200: Train Loss=0.001978, Val Loss=0.000177, Fidelity=0.9268\n",
      "Epoch 250: Train Loss=0.001959, Val Loss=0.000192, Fidelity=0.9267\n",
      "Epoch 300: Train Loss=0.001934, Val Loss=0.000179, Fidelity=0.9259\n",
      "Early stopping at epoch 318\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9261 ± 0.1220\n",
      "  RMSE (x,y,z): (0.0132, 0.0137, 0.0160)\n",
      "  Frac > 0.95: 0.7001\n",
      "\n",
      "============================================================\n",
      "Running: PriorityD_adaptive_baseline_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.040321, Val Loss=0.001485, Fidelity=0.9156\n",
      "Epoch 50: Train Loss=0.002465, Val Loss=0.000280, Fidelity=0.9247\n",
      "Epoch 100: Train Loss=0.002119, Val Loss=0.000270, Fidelity=0.9231\n",
      "Epoch 150: Train Loss=0.002012, Val Loss=0.000248, Fidelity=0.9230\n",
      "Epoch 200: Train Loss=0.001955, Val Loss=0.000275, Fidelity=0.9223\n",
      "Early stopping at epoch 218\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9228 ± 0.1262\n",
      "  RMSE (x,y,z): (0.0195, 0.0152, 0.0167)\n",
      "  Frac > 0.95: 0.7015\n",
      "\n",
      "============================================================\n",
      "Running: PriorityD_adaptive_baseline_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.040404, Val Loss=0.001196, Fidelity=0.9160\n",
      "Epoch 50: Train Loss=0.002428, Val Loss=0.000261, Fidelity=0.9235\n",
      "Epoch 100: Train Loss=0.002106, Val Loss=0.000269, Fidelity=0.9227\n",
      "Epoch 150: Train Loss=0.001993, Val Loss=0.000255, Fidelity=0.9217\n",
      "Early stopping at epoch 184\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.9232 ± 0.1258\n",
      "  RMSE (x,y,z): (0.0176, 0.0142, 0.0153)\n",
      "  Frac > 0.95: 0.7009\n",
      "\n",
      "============================================================\n",
      "Running: PriorityD_adaptive_XZ_seed48\n",
      "Seed: 48\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.100912, Val Loss=0.070233, Fidelity=0.8151\n",
      "Epoch 50: Train Loss=0.071915, Val Loss=0.069369, Fidelity=0.8253\n",
      "Epoch 100: Train Loss=0.071796, Val Loss=0.069324, Fidelity=0.8244\n",
      "Epoch 150: Train Loss=0.071803, Val Loss=0.069408, Fidelity=0.8253\n",
      "Epoch 200: Train Loss=0.071773, Val Loss=0.069360, Fidelity=0.8237\n",
      "Early stopping at epoch 224\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8210 ± 0.1568\n",
      "  RMSE (x,y,z): (0.0136, 0.4639, 0.0169)\n",
      "  Frac > 0.95: 0.3500\n",
      "\n",
      "============================================================\n",
      "Running: PriorityD_adaptive_XZ_seed49\n",
      "Seed: 49\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.097702, Val Loss=0.071616, Fidelity=0.8124\n",
      "Epoch 50: Train Loss=0.073011, Val Loss=0.070980, Fidelity=0.8207\n",
      "Epoch 100: Train Loss=0.072888, Val Loss=0.070972, Fidelity=0.8213\n",
      "Epoch 150: Train Loss=0.072824, Val Loss=0.070962, Fidelity=0.8212\n",
      "Epoch 200: Train Loss=0.072787, Val Loss=0.070982, Fidelity=0.8190\n",
      "Early stopping at epoch 213\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8237 ± 0.1573\n",
      "  RMSE (x,y,z): (0.0129, 0.4583, 0.0159)\n",
      "  Frac > 0.95: 0.3562\n",
      "\n",
      "============================================================\n",
      "Running: PriorityD_adaptive_XZ_seed50\n",
      "Seed: 50\n",
      "============================================================\n",
      "Generating training data...\n",
      "Generating validation data...\n",
      "Generating test data...\n",
      "Training model...\n",
      "Epoch 0: Train Loss=0.095812, Val Loss=0.072356, Fidelity=0.8118\n",
      "Epoch 50: Train Loss=0.072414, Val Loss=0.071359, Fidelity=0.8185\n",
      "Epoch 100: Train Loss=0.072365, Val Loss=0.071315, Fidelity=0.8186\n",
      "Epoch 150: Train Loss=0.072314, Val Loss=0.071287, Fidelity=0.8173\n",
      "Epoch 200: Train Loss=0.072277, Val Loss=0.071321, Fidelity=0.8159\n",
      "Epoch 250: Train Loss=0.072305, Val Loss=0.071308, Fidelity=0.8166\n",
      "Epoch 300: Train Loss=0.072298, Val Loss=0.071283, Fidelity=0.8173\n",
      "Epoch 350: Train Loss=0.072255, Val Loss=0.071342, Fidelity=0.8149\n",
      "Epoch 400: Train Loss=0.072201, Val Loss=0.071348, Fidelity=0.8151\n",
      "Early stopping at epoch 436\n",
      "Evaluating on test set...\n",
      "\n",
      "Results:\n",
      "  Mean Fidelity: 0.8203 ± 0.1562\n",
      "  RMSE (x,y,z): (0.0150, 0.4558, 0.0158)\n",
      "  Frac > 0.95: 0.3432\n",
      "\n",
      "NOTE: Full adaptive measurement would require sequential decision-making.\n",
      "This experiment compares measurement efficiency as a proxy.\n",
      "\n",
      "Priority D completed: 6 experiments\n",
      "\n",
      "Generating visualizations...\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2713' in position 0: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1059\u001b[39m\n\u001b[32m   1056\u001b[39m     plot_bloch_sphere_failures(results_a[\u001b[32m0\u001b[39m], OUTPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mfailure_examples.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1058\u001b[39m \u001b[38;5;66;03m# Generate final report\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m \u001b[43mgenerate_final_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m   1062\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mALL EXPERIMENTS COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 947\u001b[39m, in \u001b[36mgenerate_final_report\u001b[39m\u001b[34m(all_results, output_dir)\u001b[39m\n\u001b[32m    945\u001b[39m         f.write(\u001b[33m\"\u001b[39m\u001b[33m    This exceeds expected performance and should be investigated.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    946\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m✓  2-basis performance reasonable (max: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmax_2basis\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m f.write(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    950\u001b[39m f.write(\u001b[33m\"\u001b[39m\u001b[33mFILES GENERATED\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\encodings\\cp1252.py:19\u001b[39m, in \u001b[36mIncrementalEncoder.encode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'charmap' codec can't encode character '\\u2713' in position 0: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quantum State Tomography with Neural Networks\n",
    "Complete experimental suite with seed 48\n",
    "Results saved to expt_3/\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import unitary_group\n",
    "import pandas as pd\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = Path(\"expt_3\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "MASTER_SEED = 48\n",
    "np.random.seed(MASTER_SEED)\n",
    "torch.manual_seed(MASTER_SEED)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Master seed: {MASTER_SEED}\")\n",
    "\n",
    "###########################################\n",
    "# 1. DATA GENERATION\n",
    "###########################################\n",
    "\n",
    "class QuantumStateGenerator:\n",
    "    \"\"\"Generate random quantum states and density matrices\"\"\"\n",
    "    \n",
    "    def __init__(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def generate_pure_state(self):\n",
    "        \"\"\"Generate random pure state\"\"\"\n",
    "        # Random point on Bloch sphere\n",
    "        theta = np.random.uniform(0, np.pi)\n",
    "        phi = np.random.uniform(0, 2*np.pi)\n",
    "        \n",
    "        state = np.array([\n",
    "            np.cos(theta/2),\n",
    "            np.exp(1j*phi) * np.sin(theta/2)\n",
    "        ], dtype=complex)\n",
    "        \n",
    "        rho = np.outer(state, state.conj())\n",
    "        return rho, np.array([np.sin(theta)*np.cos(phi), \n",
    "                              np.sin(theta)*np.sin(phi), \n",
    "                              np.cos(theta)])\n",
    "    \n",
    "    def generate_mixed_state(self, p):\n",
    "        \"\"\"Generate mixed state with mixing parameter p\"\"\"\n",
    "        # Pure state component\n",
    "        rho_pure, _ = self.generate_pure_state()\n",
    "        \n",
    "        # Maximally mixed state\n",
    "        rho_mixed = np.eye(2) / 2\n",
    "        \n",
    "        # Mix them\n",
    "        rho = (1-p) * rho_pure + p * rho_mixed\n",
    "        \n",
    "        # Extract Bloch vector\n",
    "        bloch = self.density_to_bloch(rho)\n",
    "        return rho, bloch\n",
    "    \n",
    "    def generate_near_pure_state(self, eigenvalues=[0.99, 0.01]):\n",
    "        \"\"\"Generate near-pure state with specified eigenvalues\"\"\"\n",
    "        # Random unitary\n",
    "        U = unitary_group.rvs(2)\n",
    "        \n",
    "        # Construct density matrix\n",
    "        rho = U @ np.diag(eigenvalues) @ U.conj().T\n",
    "        bloch = self.density_to_bloch(rho)\n",
    "        \n",
    "        return rho, bloch\n",
    "    \n",
    "    def density_to_bloch(self, rho):\n",
    "        \"\"\"Convert density matrix to Bloch vector\"\"\"\n",
    "        pauli_x = np.array([[0, 1], [1, 0]])\n",
    "        pauli_y = np.array([[0, -1j], [1j, 0]])\n",
    "        pauli_z = np.array([[1, 0], [0, -1]])\n",
    "        \n",
    "        x = np.real(np.trace(rho @ pauli_x))\n",
    "        y = np.real(np.trace(rho @ pauli_y))\n",
    "        z = np.real(np.trace(rho @ pauli_z))\n",
    "        \n",
    "        return np.array([x, y, z])\n",
    "    \n",
    "    def bloch_to_density(self, bloch):\n",
    "        \"\"\"Convert Bloch vector to density matrix\"\"\"\n",
    "        x, y, z = bloch\n",
    "        rho = 0.5 * np.array([\n",
    "            [1 + z, x - 1j*y],\n",
    "            [x + 1j*y, 1 - z]\n",
    "        ], dtype=complex)\n",
    "        return rho\n",
    "\n",
    "\n",
    "class MeasurementSimulator:\n",
    "    \"\"\"Simulate quantum measurements with noise\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pauli_x = np.array([[0, 1], [1, 0]])\n",
    "        self.pauli_y = np.array([[0, -1j], [1j, 0]])\n",
    "        self.pauli_z = np.array([[1, 0], [0, -1]])\n",
    "    \n",
    "    def measure_pauli(self, rho, pauli):\n",
    "        \"\"\"Measure expectation value of Pauli operator\"\"\"\n",
    "        return np.real(np.trace(rho @ pauli))\n",
    "    \n",
    "    def measure_xyz(self, rho):\n",
    "        \"\"\"Measure all three Pauli operators\"\"\"\n",
    "        return np.array([\n",
    "            self.measure_pauli(rho, self.pauli_x),\n",
    "            self.measure_pauli(rho, self.pauli_y),\n",
    "            self.measure_pauli(rho, self.pauli_z)\n",
    "        ])\n",
    "    \n",
    "    def measure_two_basis(self, rho, basis_pair):\n",
    "        \"\"\"Measure two Pauli bases\"\"\"\n",
    "        paulis = {\n",
    "            'X': self.pauli_x,\n",
    "            'Y': self.pauli_y,\n",
    "            'Z': self.pauli_z\n",
    "        }\n",
    "        \n",
    "        return np.array([\n",
    "            self.measure_pauli(rho, paulis[basis_pair[0]]),\n",
    "            self.measure_pauli(rho, paulis[basis_pair[1]])\n",
    "        ])\n",
    "    \n",
    "    def measure_sic_povm(self, rho):\n",
    "        \"\"\"Simulate SIC-POVM measurement (4 outcomes)\"\"\"\n",
    "        # SIC-POVM elements for qubit\n",
    "        sic_elements = [\n",
    "            np.array([[1, 0], [0, 0]]) / 2,  # |0><0|\n",
    "            np.array([[1, 1], [1, 1]]) / 4,  # (|0>+|1>)(|0>+|1>)*\n",
    "            np.array([[1, -1j], [1j, 1]]) / 4,\n",
    "            np.array([[1, 1j], [-1j, 1]]) / 4\n",
    "        ]\n",
    "        \n",
    "        probs = [np.real(np.trace(rho @ elem)) for elem in sic_elements]\n",
    "        return np.array(probs)\n",
    "    \n",
    "    def apply_shot_noise(self, probabilities, shots):\n",
    "        \"\"\"Convert probabilities to finite-shot counts\"\"\"\n",
    "        # For Pauli measurements: probability of +1 outcome\n",
    "        prob_plus = (probabilities + 1) / 2\n",
    "        counts_plus = np.random.binomial(shots, prob_plus)\n",
    "        \n",
    "        # Convert back to expectation values\n",
    "        return 2 * counts_plus / shots - 1\n",
    "    \n",
    "    def apply_readout_noise(self, probabilities, noise_level):\n",
    "        \"\"\"Apply readout flip noise\"\"\"\n",
    "        # Noise matrix: probability of bit flip\n",
    "        # |0> -> |1> with probability noise_level\n",
    "        # |1> -> |0> with probability noise_level\n",
    "        \n",
    "        prob_plus = (probabilities + 1) / 2\n",
    "        \n",
    "        # Apply noise\n",
    "        noisy_prob = prob_plus * (1 - noise_level) + (1 - prob_plus) * noise_level\n",
    "        \n",
    "        # Convert back to expectation values\n",
    "        return 2 * noisy_prob - 1\n",
    "    \n",
    "    def measure_with_noise(self, rho, measurement_type, shots=None, noise_level=0):\n",
    "        \"\"\"Measure with optional shot noise and readout noise\"\"\"\n",
    "        \n",
    "        if measurement_type == 'baseline':\n",
    "            probs = self.measure_xyz(rho)\n",
    "        elif measurement_type in ['XY', 'XZ', 'YZ']:\n",
    "            probs = self.measure_two_basis(rho, measurement_type)\n",
    "        elif measurement_type == 'sic':\n",
    "            return self.measure_sic_povm(rho)  # SIC is different format\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown measurement type: {measurement_type}\")\n",
    "        \n",
    "        # Apply shot noise if specified\n",
    "        if shots is not None:\n",
    "            probs = self.apply_shot_noise(probs, shots)\n",
    "        \n",
    "        # Apply readout noise\n",
    "        if noise_level > 0:\n",
    "            probs = self.apply_readout_noise(probs, noise_level)\n",
    "        \n",
    "        return probs\n",
    "\n",
    "\n",
    "def generate_dataset(n_states, ensemble_type='general', mixing_p=0.25, \n",
    "                     measurement_type='baseline', shots=None, noise_level=0, seed=None):\n",
    "    \"\"\"\n",
    "    Generate complete dataset\n",
    "    \n",
    "    Args:\n",
    "        n_states: number of states to generate\n",
    "        ensemble_type: 'general', 'pure', 'near_pure', 'mixed'\n",
    "        mixing_p: mixing parameter for mixed states\n",
    "        measurement_type: 'baseline', 'XY', 'XZ', 'YZ', 'sic'\n",
    "        shots: number of shots (None for infinite)\n",
    "        noise_level: readout noise level (0-1)\n",
    "        seed: random seed\n",
    "    \"\"\"\n",
    "    gen = QuantumStateGenerator(seed=seed)\n",
    "    sim = MeasurementSimulator()\n",
    "    \n",
    "    measurements = []\n",
    "    bloch_vectors = []\n",
    "    \n",
    "    for _ in range(n_states):\n",
    "        # Generate state based on ensemble type\n",
    "        if ensemble_type == 'pure':\n",
    "            rho, bloch = gen.generate_pure_state()\n",
    "        elif ensemble_type == 'near_pure':\n",
    "            rho, bloch = gen.generate_near_pure_state([0.99, 0.01])\n",
    "        elif ensemble_type == 'mixed':\n",
    "            rho, bloch = gen.generate_mixed_state(mixing_p)\n",
    "        else:  # general ensemble\n",
    "            rand = np.random.rand()\n",
    "            if rand < 0.7:  # 70% pure\n",
    "                rho, bloch = gen.generate_pure_state()\n",
    "            else:  # 30% mixed\n",
    "                rho, bloch = gen.generate_mixed_state(np.random.uniform(0.1, 0.5))\n",
    "        \n",
    "        # Perform measurement\n",
    "        meas = sim.measure_with_noise(rho, measurement_type, shots, noise_level)\n",
    "        \n",
    "        measurements.append(meas)\n",
    "        bloch_vectors.append(bloch)\n",
    "    \n",
    "    return np.array(measurements), np.array(bloch_vectors)\n",
    "\n",
    "\n",
    "###########################################\n",
    "# 2. NEURAL NETWORK MODEL\n",
    "###########################################\n",
    "\n",
    "class TomographyNet(nn.Module):\n",
    "    \"\"\"Neural network for quantum state tomography\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64, 32]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer: 3 Bloch coordinates\n",
    "        layers.append(nn.Linear(prev_dim, 3))\n",
    "        layers.append(nn.Tanh())  # Bloch vector constrained to [-1, 1]\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bloch = self.network(x)\n",
    "        # Enforce Bloch sphere constraint: ||r|| <= 1\n",
    "        norm = torch.norm(bloch, dim=1, keepdim=True)\n",
    "        bloch = bloch / torch.clamp(norm, min=1.0)\n",
    "        return bloch\n",
    "\n",
    "\n",
    "class QuantumDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for quantum measurements\"\"\"\n",
    "    \n",
    "    def __init__(self, measurements, bloch_vectors):\n",
    "        self.measurements = torch.FloatTensor(measurements)\n",
    "        self.bloch_vectors = torch.FloatTensor(bloch_vectors)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.measurements)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.measurements[idx], self.bloch_vectors[idx]\n",
    "\n",
    "\n",
    "def fidelity_metric(pred_bloch, true_bloch):\n",
    "    \"\"\"Calculate fidelity between predicted and true states\"\"\"\n",
    "    # For pure states: F = (1 + r1·r2)/2\n",
    "    # For general states, this is approximate\n",
    "    dot_product = np.sum(pred_bloch * true_bloch, axis=1)\n",
    "    fidelity = (1 + dot_product) / 2\n",
    "    return fidelity\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=1000, lr=1e-3, \n",
    "                patience=100, device='cpu'):\n",
    "    \"\"\"Train the tomography model\"\"\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_fidelities = []\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for measurements, bloch in train_loader:\n",
    "            measurements, bloch = measurements.to(device), bloch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(measurements)\n",
    "            loss = criterion(pred, bloch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for measurements, bloch in val_loader:\n",
    "                measurements, bloch = measurements.to(device), bloch.to(device)\n",
    "                pred = model(measurements)\n",
    "                loss = criterion(pred, bloch)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                all_preds.append(pred.cpu().numpy())\n",
    "                all_true.append(bloch.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate fidelity\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_true = np.vstack(all_true)\n",
    "        fidelities = fidelity_metric(all_preds, all_true)\n",
    "        mean_fidelity = np.mean(fidelities)\n",
    "        val_fidelities.append(mean_fidelity)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}, Fidelity={mean_fidelity:.4f}\")\n",
    "    \n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_fidelities': val_fidelities,\n",
    "        'best_epoch': len(train_losses) - patience,\n",
    "        'final_fidelity': val_fidelities[-patience] if len(val_fidelities) > patience else val_fidelities[-1]\n",
    "    }\n",
    "\n",
    "\n",
    "###########################################\n",
    "# 3. EXPERIMENT RUNNERS\n",
    "###########################################\n",
    "\n",
    "def run_single_experiment(config, seed):\n",
    "    \"\"\"Run a single experiment with given configuration\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {config['name']}\")\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate data\n",
    "    print(\"Generating training data...\")\n",
    "    train_meas, train_bloch = generate_dataset(\n",
    "        n_states=config['n_train'],\n",
    "        ensemble_type=config.get('ensemble_type', 'general'),\n",
    "        mixing_p=config.get('mixing_p', 0.25),\n",
    "        measurement_type=config['measurement_type'],\n",
    "        shots=config.get('shots', None),\n",
    "        noise_level=config.get('noise_level', 0),\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    print(\"Generating validation data...\")\n",
    "    val_meas, val_bloch = generate_dataset(\n",
    "        n_states=config['n_val'],\n",
    "        ensemble_type=config.get('ensemble_type', 'general'),\n",
    "        mixing_p=config.get('mixing_p', 0.25),\n",
    "        measurement_type=config['measurement_type'],\n",
    "        shots=config.get('shots', None),\n",
    "        noise_level=config.get('noise_level', 0),\n",
    "        seed=seed + 1000\n",
    "    )\n",
    "    \n",
    "    print(\"Generating test data...\")\n",
    "    test_meas, test_bloch = generate_dataset(\n",
    "        n_states=config['n_test'],\n",
    "        ensemble_type=config.get('ensemble_type', 'general'),\n",
    "        mixing_p=config.get('mixing_p', 0.25),\n",
    "        measurement_type=config['measurement_type'],\n",
    "        shots=config.get('shots', None),\n",
    "        noise_level=config.get('noise_level', 0),\n",
    "        seed=seed + 2000\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = QuantumDataset(train_meas, train_bloch)\n",
    "    val_dataset = QuantumDataset(val_meas, val_bloch)\n",
    "    test_dataset = QuantumDataset(test_meas, test_bloch)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=512)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512)\n",
    "    \n",
    "    # Create model\n",
    "    input_dim = train_meas.shape[1]\n",
    "    model = TomographyNet(input_dim=input_dim, hidden_dims=[256, 128, 64, 32])\n",
    "    \n",
    "    # Train\n",
    "    print(\"Training model...\")\n",
    "    history = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        epochs=1000, lr=1e-3, patience=100,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for measurements, bloch in test_loader:\n",
    "            measurements = measurements.to(device)\n",
    "            pred = model(measurements)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_true.append(bloch.numpy())\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_true = np.vstack(all_true)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fidelities = fidelity_metric(all_preds, all_true)\n",
    "    rmse = np.sqrt(np.mean((all_preds - all_true)**2, axis=0))\n",
    "    \n",
    "    results = {\n",
    "        'config': config,\n",
    "        'seed': seed,\n",
    "        'history': history,\n",
    "        'test_fidelity_mean': np.mean(fidelities),\n",
    "        'test_fidelity_std': np.std(fidelities),\n",
    "        'test_fidelity_distribution': fidelities,\n",
    "        'rmse_x': rmse[0],\n",
    "        'rmse_y': rmse[1],\n",
    "        'rmse_z': rmse[2],\n",
    "        'frac_above_95': np.mean(fidelities > 0.95),\n",
    "        'predictions': all_preds,\n",
    "        'true_values': all_true\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Mean Fidelity: {results['test_fidelity_mean']:.4f} ± {results['test_fidelity_std']:.4f}\")\n",
    "    print(f\"  RMSE (x,y,z): ({rmse[0]:.4f}, {rmse[1]:.4f}, {rmse[2]:.4f})\")\n",
    "    print(f\"  Frac > 0.95: {results['frac_above_95']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_priority_a():\n",
    "    \"\"\"Priority A: Finite shots + readout noise\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRIORITY A: Finite Shots + Readout Noise\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = []\n",
    "    seeds = [48, 49, 50]\n",
    "    \n",
    "    for shots in [10, 100, 1000]:\n",
    "        for noise in [0, 0.01, 0.05]:\n",
    "            for measurement_type in ['baseline', 'XZ']:\n",
    "                for seed in seeds:\n",
    "                    config = {\n",
    "                        'name': f'PriorityA_shots{shots}_noise{int(noise*100)}pct_{measurement_type}_seed{seed}',\n",
    "                        'n_train': 80000,\n",
    "                        'n_val': 10000,\n",
    "                        'n_test': 10000,\n",
    "                        'measurement_type': measurement_type,\n",
    "                        'shots': shots,\n",
    "                        'noise_level': noise\n",
    "                    }\n",
    "                    \n",
    "                    results = run_single_experiment(config, seed)\n",
    "                    all_results.append(results)\n",
    "                    \n",
    "                    # Save individual result\n",
    "                    save_path = OUTPUT_DIR / f\"{config['name']}.pkl\"\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        pickle.dump(results, f)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def run_priority_b():\n",
    "    \"\"\"Priority B: Pure vs mixed vs near-pure ensembles\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRIORITY B: Pure vs Mixed vs Near-Pure\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = []\n",
    "    seeds = [48, 49, 50]\n",
    "    \n",
    "    ensembles = [\n",
    "        ('pure', None),\n",
    "        ('near_pure', None),\n",
    "        ('mixed', 0.1),\n",
    "        ('mixed', 0.25),\n",
    "        ('mixed', 0.5)\n",
    "    ]\n",
    "    \n",
    "    for ensemble_type, mixing_p in ensembles:\n",
    "        for measurement_type in ['baseline', 'XY', 'XZ', 'YZ']:\n",
    "            for seed in seeds:\n",
    "                ensemble_name = f\"{ensemble_type}_p{mixing_p}\" if mixing_p else ensemble_type\n",
    "                config = {\n",
    "                    'name': f'PriorityB_{ensemble_name}_{measurement_type}_seed{seed}',\n",
    "                    'n_train': 80000,\n",
    "                    'n_val': 10000,\n",
    "                    'n_test': 10000,\n",
    "                    'ensemble_type': ensemble_type,\n",
    "                    'mixing_p': mixing_p,\n",
    "                    'measurement_type': measurement_type\n",
    "                }\n",
    "                \n",
    "                results = run_single_experiment(config, seed)\n",
    "                all_results.append(results)\n",
    "                \n",
    "                # Save individual result\n",
    "                save_path = OUTPUT_DIR / f\"{config['name']}.pkl\"\n",
    "                with open(save_path, 'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "###########################################\n",
    "# 4. ANALYSIS AND VISUALIZATION\n",
    "###########################################\n",
    "\n",
    "def create_summary_table(results_list):\n",
    "    \"\"\"Create summary table of all results\"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    for r in results_list:\n",
    "        rows.append({\n",
    "            'Experiment': r['config']['name'],\n",
    "            'Mean Fidelity': r['test_fidelity_mean'],\n",
    "            'Std Fidelity': r['test_fidelity_std'],\n",
    "            'RMSE_x': r['rmse_x'],\n",
    "            'RMSE_y': r['rmse_y'],\n",
    "            'RMSE_z': r['rmse_z'],\n",
    "            'Frac > 0.95': r['frac_above_95'],\n",
    "            'Convergence Epoch': r['history']['best_epoch']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_training_curves(results, save_path):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0].plot(results['history']['train_losses'], label='Train Loss', alpha=0.7)\n",
    "    axes[0].plot(results['history']['val_losses'], label='Val Loss', alpha=0.7)\n",
    "    axes[0].axvline(results['history']['best_epoch'], color='r', linestyle='--', label='Best Epoch')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('MSE Loss')\n",
    "    axes[0].set_title('Training Curves')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Fidelity curve\n",
    "    axes[1].plot(results['history']['val_fidelities'], label='Val Fidelity', color='green', alpha=0.7)\n",
    "    axes[1].axvline(results['history']['best_epoch'], color='r', linestyle='--', label='Best Epoch')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Mean Fidelity')\n",
    "    axes[1].set_title('Validation Fidelity')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_fidelity_cdf(results_list, save_path, title=\"Fidelity CDF\"):\n",
    "    \"\"\"Plot CDF of fidelities for multiple experiments\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for r in results_list:\n",
    "        fidelities = np.sort(r['test_fidelity_distribution'])\n",
    "        cdf = np.arange(1, len(fidelities) + 1) / len(fidelities)\n",
    "        label = r['config']['name'].replace('PriorityA_', '').replace('PriorityB_', '')\n",
    "        plt.plot(fidelities, cdf, label=label, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Fidelity')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_bloch_sphere_failures(results, save_path, n_examples=5):\n",
    "    \"\"\"Plot worst predictions on Bloch sphere\"\"\"\n",
    "    \n",
    "    fidelities = fidelity_metric(results['predictions'], results['true_values'])\n",
    "    worst_indices = np.argsort(fidelities)[:n_examples]\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 3))\n",
    "    \n",
    "    for i, idx in enumerate(worst_indices):\n",
    "        ax = fig.add_subplot(1, n_examples, i+1, projection='3d')\n",
    "        \n",
    "        true_vec = results['true_values'][idx]\n",
    "        pred_vec = results['predictions'][idx]\n",
    "        \n",
    "        # Draw Bloch sphere\n",
    "        u = np.linspace(0, 2*np.pi, 50)\n",
    "        v = np.linspace(0, np.pi, 50)\n",
    "        x = np.outer(np.cos(u), np.sin(v))\n",
    "        y = np.outer(np.sin(u), np.sin(v))\n",
    "        z = np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "        ax.plot_surface(x, y, z, alpha=0.1, color='gray')\n",
    "        \n",
    "        # Plot vectors\n",
    "        ax.quiver(0, 0, 0, true_vec[0], true_vec[1], true_vec[2], \n",
    "                 color='blue', arrow_length_ratio=0.1, linewidth=2, label='True')\n",
    "        ax.quiver(0, 0, 0, pred_vec[0], pred_vec[1], pred_vec[2], \n",
    "                 color='red', arrow_length_ratio=0.1, linewidth=2, label='Predicted')\n",
    "        \n",
    "        ax.set_xlim([-1, 1])\n",
    "        ax.set_ylim([-1, 1])\n",
    "        ax.set_zlim([-1, 1])\n",
    "        ax.set_title(f'F={fidelities[idx]:.3f}')\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "###########################################\n",
    "# 5. MAIN EXECUTION\n",
    "###########################################\n",
    "\n",
    "def run_priority_c():\n",
    "    \"\"\"Priority C: SIC-POVM vs Pauli 2-basis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRIORITY C: SIC-POVM Input Test\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = []\n",
    "    seeds = [48, 49, 50]\n",
    "    \n",
    "    for measurement_type in ['sic', 'XZ']:\n",
    "        for seed in seeds:\n",
    "            config = {\n",
    "                'name': f'PriorityC_{measurement_type}_seed{seed}',\n",
    "                'n_train': 80000,\n",
    "                'n_val': 10000,\n",
    "                'n_test': 10000,\n",
    "                'measurement_type': measurement_type\n",
    "            }\n",
    "            \n",
    "            results = run_single_experiment(config, seed)\n",
    "            all_results.append(results)\n",
    "            \n",
    "            # Save individual result\n",
    "            save_path = OUTPUT_DIR / f\"{config['name']}.pkl\"\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def run_priority_d():\n",
    "    \"\"\"Priority D: Adaptive vs non-adaptive measurement\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRIORITY D: Adaptive Measurement Protocol\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # For this simplified version, we'll simulate adaptive by:\n",
    "    # 1. Start with X measurement\n",
    "    # 2. Choose next axis based on information gain heuristic\n",
    "    # This is a simplified adaptive protocol\n",
    "    \n",
    "    all_results = []\n",
    "    seeds = [48, 49, 50]\n",
    "    \n",
    "    # We'll compare baseline (all 3 axes) vs adaptive (2 axes chosen smartly)\n",
    "    # For now, just compare baseline vs best 2-basis as proxy\n",
    "    \n",
    "    for measurement_type in ['baseline', 'XZ']:\n",
    "        for seed in seeds:\n",
    "            config = {\n",
    "                'name': f'PriorityD_adaptive_{measurement_type}_seed{seed}',\n",
    "                'n_train': 80000,\n",
    "                'n_val': 10000,\n",
    "                'n_test': 10000,\n",
    "                'measurement_type': measurement_type\n",
    "            }\n",
    "            \n",
    "            results = run_single_experiment(config, seed)\n",
    "            all_results.append(results)\n",
    "            \n",
    "            # Save individual result\n",
    "            save_path = OUTPUT_DIR / f\"{config['name']}.pkl\"\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "    \n",
    "    print(\"\\nNOTE: Full adaptive measurement would require sequential decision-making.\")\n",
    "    print(\"This experiment compares measurement efficiency as a proxy.\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def aggregate_results_by_config(results_list):\n",
    "    \"\"\"Aggregate results across seeds for same configuration\"\"\"\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    grouped = defaultdict(list)\n",
    "    \n",
    "    for r in results_list:\n",
    "        # Extract config key (everything except seed)\n",
    "        name_parts = r['config']['name'].rsplit('_seed', 1)\n",
    "        config_key = name_parts[0]\n",
    "        grouped[config_key].append(r)\n",
    "    \n",
    "    summary = []\n",
    "    for config_key, results in grouped.items():\n",
    "        fidelities = [r['test_fidelity_mean'] for r in results]\n",
    "        \n",
    "        summary.append({\n",
    "            'Configuration': config_key,\n",
    "            'Mean Fidelity': np.mean(fidelities),\n",
    "            'Std across seeds': np.std(fidelities),\n",
    "            'Min Fidelity': np.min(fidelities),\n",
    "            'Max Fidelity': np.max(fidelities),\n",
    "            'N Seeds': len(results)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "def plot_comparison_bar(df, save_path, title, y_col='Mean Fidelity'):\n",
    "    \"\"\"Create bar plot comparing configurations\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(df))\n",
    "    plt.bar(x, df[y_col], yerr=df.get('Std across seeds', 0), \n",
    "            alpha=0.7, capsize=5)\n",
    "    \n",
    "    plt.xticks(x, df['Configuration'], rotation=45, ha='right')\n",
    "    plt.ylabel(y_col)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_noise_vs_performance(results_a, save_path):\n",
    "    \"\"\"Plot how performance degrades with noise and finite shots\"\"\"\n",
    "    \n",
    "    # Extract data\n",
    "    data = []\n",
    "    for r in results_a:\n",
    "        config = r['config']\n",
    "        if 'shots' in config and 'noise_level' in config:\n",
    "            data.append({\n",
    "                'shots': config['shots'],\n",
    "                'noise': config['noise_level'] * 100,\n",
    "                'measurement': config['measurement_type'],\n",
    "                'fidelity': r['test_fidelity_mean'],\n",
    "                'seed': r['seed']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"No noise/shots data to plot\")\n",
    "        return\n",
    "    \n",
    "    # Aggregate across seeds\n",
    "    df_agg = df.groupby(['shots', 'noise', 'measurement']).agg({\n",
    "        'fidelity': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "    df_agg.columns = ['shots', 'noise', 'measurement', 'fidelity_mean', 'fidelity_std']\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for meas_type in df_agg['measurement'].unique():\n",
    "        df_meas = df_agg[df_agg['measurement'] == meas_type]\n",
    "        \n",
    "        # Plot vs shots\n",
    "        for noise in df_meas['noise'].unique():\n",
    "            df_noise = df_meas[df_meas['noise'] == noise]\n",
    "            axes[0].errorbar(df_noise['shots'], df_noise['fidelity_mean'],\n",
    "                           yerr=df_noise['fidelity_std'], \n",
    "                           marker='o', label=f'{meas_type}, {noise}% noise',\n",
    "                           capsize=5, alpha=0.7)\n",
    "        \n",
    "        # Plot vs noise\n",
    "        for shots in df_meas['shots'].unique():\n",
    "            df_shots = df_meas[df_meas['shots'] == shots]\n",
    "            axes[1].errorbar(df_shots['noise'], df_shots['fidelity_mean'],\n",
    "                           yerr=df_shots['fidelity_std'],\n",
    "                           marker='o', label=f'{meas_type}, {shots} shots',\n",
    "                           capsize=5, alpha=0.7)\n",
    "    \n",
    "    axes[0].set_xlabel('Number of Shots')\n",
    "    axes[0].set_ylabel('Mean Fidelity')\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_title('Performance vs Shot Budget')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_xlabel('Readout Noise (%)')\n",
    "    axes[1].set_ylabel('Mean Fidelity')\n",
    "    axes[1].set_title('Performance vs Readout Noise')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_final_report(all_results, output_dir):\n",
    "    \"\"\"Generate comprehensive final report\"\"\"\n",
    "    \n",
    "    report_path = output_dir / \"FINAL_REPORT.txt\"\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"QUANTUM STATE TOMOGRAPHY - FINAL EXPERIMENTAL REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Date: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"Master Seed: {MASTER_SEED}\\n\")\n",
    "        f.write(f\"Total Experiments: {len(all_results)}\\n\")\n",
    "        f.write(f\"Output Directory: {output_dir}\\n\\n\")\n",
    "        \n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"OVERALL STATISTICS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        all_fidelities = [r['test_fidelity_mean'] for r in all_results]\n",
    "        f.write(f\"Mean Fidelity across all experiments: {np.mean(all_fidelities):.4f} ± {np.std(all_fidelities):.4f}\\n\")\n",
    "        f.write(f\"Best Fidelity: {np.max(all_fidelities):.4f}\\n\")\n",
    "        f.write(f\"Worst Fidelity: {np.min(all_fidelities):.4f}\\n\")\n",
    "        \n",
    "        # Count high-performing experiments\n",
    "        high_perf = sum(1 for f in all_fidelities if f > 0.90)\n",
    "        f.write(f\"\\nExperiments with fidelity > 0.90: {high_perf}/{len(all_results)} ({100*high_perf/len(all_results):.1f}%)\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "        f.write(\"KEY FINDINGS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        # Group by priority\n",
    "        priorities = {}\n",
    "        for r in all_results:\n",
    "            if 'PriorityA' in r['config']['name']:\n",
    "                priorities.setdefault('A', []).append(r)\n",
    "            elif 'PriorityB' in r['config']['name']:\n",
    "                priorities.setdefault('B', []).append(r)\n",
    "            elif 'PriorityC' in r['config']['name']:\n",
    "                priorities.setdefault('C', []).append(r)\n",
    "            elif 'PriorityD' in r['config']['name']:\n",
    "                priorities.setdefault('D', []).append(r)\n",
    "        \n",
    "        for priority, results in sorted(priorities.items()):\n",
    "            fids = [r['test_fidelity_mean'] for r in results]\n",
    "            f.write(f\"\\nPriority {priority}: {len(results)} experiments\\n\")\n",
    "            f.write(f\"  Mean Fidelity: {np.mean(fids):.4f} ± {np.std(fids):.4f}\\n\")\n",
    "            f.write(f\"  Range: [{np.min(fids):.4f}, {np.max(fids):.4f}]\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "        f.write(\"ANOMALY CHECKS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        # Check for unexpectedly high 2-basis performance\n",
    "        two_basis_results = [r for r in all_results if any(x in r['config']['measurement_type'] \n",
    "                                                            for x in ['XY', 'XZ', 'YZ'])]\n",
    "        if two_basis_results:\n",
    "            two_basis_fids = [r['test_fidelity_mean'] for r in two_basis_results]\n",
    "            max_2basis = np.max(two_basis_fids)\n",
    "            if max_2basis > 0.90:\n",
    "                f.write(f\"⚠️  WARNING: 2-basis model achieved {max_2basis:.4f} fidelity\\n\")\n",
    "                f.write(\"    This exceeds expected performance and should be investigated.\\n\")\n",
    "            else:\n",
    "                f.write(f\"✓  2-basis performance reasonable (max: {max_2basis:.4f})\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "        f.write(\"FILES GENERATED\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        files = list(output_dir.glob(\"*\"))\n",
    "        for file in sorted(files):\n",
    "            f.write(f\"  - {file.name}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        f.write(\"END OF REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nFinal report saved to: {report_path}\")\n",
    "    \n",
    "    # Also print to console\n",
    "    with open(report_path, 'r') as f:\n",
    "        print(f.read())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"QUANTUM STATE TOMOGRAPHY EXPERIMENTS\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"Master seed: {MASTER_SEED}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Run Priority A\n",
    "    print(\"\\nStarting Priority A experiments...\")\n",
    "    results_a = run_priority_a()\n",
    "    all_results.extend(results_a)\n",
    "    \n",
    "    # Save summary\n",
    "    df_a = create_summary_table(results_a)\n",
    "    df_a.to_csv(OUTPUT_DIR / \"priority_a_summary.csv\", index=False)\n",
    "    print(f\"\\nPriority A completed: {len(results_a)} experiments\")\n",
    "    \n",
    "    # Aggregate across seeds\n",
    "    df_a_agg = aggregate_results_by_config(results_a)\n",
    "    df_a_agg.to_csv(OUTPUT_DIR / \"priority_a_aggregated.csv\", index=False)\n",
    "    \n",
    "    # Run Priority B\n",
    "    print(\"\\nStarting Priority B experiments...\")\n",
    "    results_b = run_priority_b()\n",
    "    all_results.extend(results_b)\n",
    "    \n",
    "    # Save summary\n",
    "    df_b = create_summary_table(results_b)\n",
    "    df_b.to_csv(OUTPUT_DIR / \"priority_b_summary.csv\", index=False)\n",
    "    print(f\"\\nPriority B completed: {len(results_b)} experiments\")\n",
    "    \n",
    "    # Aggregate across seeds\n",
    "    df_b_agg = aggregate_results_by_config(results_b)\n",
    "    df_b_agg.to_csv(OUTPUT_DIR / \"priority_b_aggregated.csv\", index=False)\n",
    "    \n",
    "    # Run Priority C\n",
    "    print(\"\\nStarting Priority C experiments...\")\n",
    "    results_c = run_priority_c()\n",
    "    all_results.extend(results_c)\n",
    "    \n",
    "    df_c = create_summary_table(results_c)\n",
    "    df_c.to_csv(OUTPUT_DIR / \"priority_c_summary.csv\", index=False)\n",
    "    print(f\"\\nPriority C completed: {len(results_c)} experiments\")\n",
    "    \n",
    "    df_c_agg = aggregate_results_by_config(results_c)\n",
    "    df_c_agg.to_csv(OUTPUT_DIR / \"priority_c_aggregated.csv\", index=False)\n",
    "    \n",
    "    # Run Priority D\n",
    "    print(\"\\nStarting Priority D experiments...\")\n",
    "    results_d = run_priority_d()\n",
    "    all_results.extend(results_d)\n",
    "    \n",
    "    df_d = create_summary_table(results_d)\n",
    "    df_d.to_csv(OUTPUT_DIR / \"priority_d_summary.csv\", index=False)\n",
    "    print(f\"\\nPriority D completed: {len(results_d)} experiments\")\n",
    "    \n",
    "    df_d_agg = aggregate_results_by_config(results_d)\n",
    "    df_d_agg.to_csv(OUTPUT_DIR / \"priority_d_aggregated.csv\", index=False)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    # Plot sample training curves\n",
    "    if len(results_a) > 0:\n",
    "        plot_training_curves(results_a[0], OUTPUT_DIR / \"sample_training_curves_a.png\")\n",
    "    \n",
    "    # Plot CDFs for each priority\n",
    "    plot_fidelity_cdf(results_a[:9], OUTPUT_DIR / \"priority_a_fidelity_cdf.png\", \n",
    "                      title=\"Priority A: Fidelity CDF (Finite Shots + Noise)\")\n",
    "    plot_fidelity_cdf(results_b[:12], OUTPUT_DIR / \"priority_b_fidelity_cdf.png\",\n",
    "                      title=\"Priority B: Fidelity CDF (Pure vs Mixed)\")\n",
    "    plot_fidelity_cdf(results_c, OUTPUT_DIR / \"priority_c_fidelity_cdf.png\",\n",
    "                      title=\"Priority C: SIC-POVM vs Pauli\")\n",
    "    \n",
    "    # Plot comparison bars\n",
    "    plot_comparison_bar(df_a_agg, OUTPUT_DIR / \"priority_a_comparison.png\",\n",
    "                       \"Priority A: Finite Shots + Noise Performance\")\n",
    "    plot_comparison_bar(df_b_agg, OUTPUT_DIR / \"priority_b_comparison.png\",\n",
    "                       \"Priority B: Ensemble Type Performance\")\n",
    "    \n",
    "    # Plot noise vs performance\n",
    "    plot_noise_vs_performance(results_a, OUTPUT_DIR / \"noise_degradation.png\")\n",
    "    \n",
    "    # Plot failure examples\n",
    "    if len(results_a) > 0:\n",
    "        plot_bloch_sphere_failures(results_a[0], OUTPUT_DIR / \"failure_examples.png\")\n",
    "    \n",
    "    # Generate final report\n",
    "    generate_final_report(all_results, OUTPUT_DIR)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALL EXPERIMENTS COMPLETE!\")\n",
    "    print(f\"Total experiments run: {len(all_results)}\")\n",
    "    print(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nQUICK SUMMARY:\")\n",
    "    print(f\"  Priority A: {len(results_a)} experiments\")\n",
    "    print(f\"  Priority B: {len(results_b)} experiments\")\n",
    "    print(f\"  Priority C: {len(results_c)} experiments\")\n",
    "    print(f\"  Priority D: {len(results_d)} experiments\")\n",
    "    print(f\"  Total: {len(all_results)} experiments\")\n",
    "    \n",
    "    all_fids = [r['test_fidelity_mean'] for r in all_results]\n",
    "    print(f\"\\n  Overall Mean Fidelity: {np.mean(all_fids):.4f} ± {np.std(all_fids):.4f}\")\n",
    "    print(f\"  Best Result: {np.max(all_fids):.4f}\")\n",
    "    print(f\"  Worst Result: {np.min(all_fids):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6adafc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final report saved to: expt_3\\FINAL_REPORT.txt\n",
      "================================================================================\n",
      "QUANTUM STATE TOMOGRAPHY - FINAL EXPERIMENTAL REPORT\n",
      "================================================================================\n",
      "\n",
      "Date: 2025-10-04 08:46:47.622781\n",
      "Master Seed: 48\n",
      "Total Experiments: 126\n",
      "Output Directory: expt_3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OVERALL STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Mean Fidelity across all experiments: 0.8156 +/- 0.1045\n",
      "Best Fidelity: 0.9999\n",
      "Worst Fidelity: 0.5605\n",
      "\n",
      "Experiments with fidelity > 0.90: 33/126 (26.2%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "KEY FINDINGS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Priority A: 54 experiments\n",
      "  Mean Fidelity: 0.8529 +/- 0.0559\n",
      "  Range: [0.7679, 0.9252]\n",
      "\n",
      "Priority B: 60 experiments\n",
      "  Mean Fidelity: 0.7706 +/- 0.1252\n",
      "  Range: [0.5605, 0.9999]\n",
      "\n",
      "Priority C: 6 experiments\n",
      "  Mean Fidelity: 0.8728 +/- 0.0512\n",
      "  Range: [0.8198, 0.9252]\n",
      "\n",
      "Priority D: 6 experiments\n",
      "  Mean Fidelity: 0.8729 +/- 0.0512\n",
      "  Range: [0.8203, 0.9261]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ANOMALY CHECKS\n",
      "--------------------------------------------------------------------------------\n",
      "[OK] 2-basis performance reasonable (max: 0.8812)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FILES GENERATED\n",
      "--------------------------------------------------------------------------------\n",
      "  - failure_examples.png\n",
      "  - FINAL_REPORT.txt\n",
      "  - noise_degradation.png\n",
      "  - priority_a_aggregated.csv\n",
      "  - priority_a_comparison.png\n",
      "  - priority_a_fidelity_cdf.png\n",
      "  - priority_a_summary.csv\n",
      "  - priority_b_aggregated.csv\n",
      "  - priority_b_comparison.png\n",
      "  - priority_b_fidelity_cdf.png\n",
      "  - priority_b_summary.csv\n",
      "  - priority_c_aggregated.csv\n",
      "  - priority_c_fidelity_cdf.png\n",
      "  - priority_c_summary.csv\n",
      "  - priority_d_aggregated.csv\n",
      "  - priority_d_summary.csv\n",
      "  - PriorityA_shots1000_noise0pct_baseline_seed48.pkl\n",
      "  - PriorityA_shots1000_noise0pct_baseline_seed49.pkl\n",
      "  - PriorityA_shots1000_noise0pct_baseline_seed50.pkl\n",
      "  - PriorityA_shots1000_noise0pct_XZ_seed48.pkl\n",
      "  - PriorityA_shots1000_noise0pct_XZ_seed49.pkl\n",
      "  - PriorityA_shots1000_noise0pct_XZ_seed50.pkl\n",
      "  - PriorityA_shots1000_noise1pct_baseline_seed48.pkl\n",
      "  - PriorityA_shots1000_noise1pct_baseline_seed49.pkl\n",
      "  - PriorityA_shots1000_noise1pct_baseline_seed50.pkl\n",
      "  - PriorityA_shots1000_noise1pct_XZ_seed48.pkl\n",
      "  - PriorityA_shots1000_noise1pct_XZ_seed49.pkl\n",
      "  - PriorityA_shots1000_noise1pct_XZ_seed50.pkl\n",
      "  - PriorityA_shots1000_noise5pct_baseline_seed48.pkl\n",
      "  - PriorityA_shots1000_noise5pct_baseline_seed49.pkl\n",
      "  - PriorityA_shots1000_noise5pct_baseline_seed50.pkl\n",
      "  - PriorityA_shots1000_noise5pct_XZ_seed48.pkl\n",
      "  - PriorityA_shots1000_noise5pct_XZ_seed49.pkl\n",
      "  - PriorityA_shots1000_noise5pct_XZ_seed50.pkl\n",
      "  - PriorityA_shots100_noise0pct_baseline_seed48.pkl\n",
      "  - PriorityA_shots100_noise0pct_baseline_seed49.pkl\n",
      "  - PriorityA_shots100_noise0pct_baseline_seed50.pkl\n",
      "  - PriorityA_shots100_noise0pct_XZ_seed48.pkl\n",
      "  - PriorityA_shots100_noise0pct_XZ_seed49.pkl\n",
      "  - PriorityA_shots100_noise0pct_XZ_seed50.pkl\n",
      "  - PriorityA_shots100_noise1pct_baseline_seed48.pkl\n",
      "  - PriorityA_shots100_noise1pct_baseline_seed49.pkl\n",
      "  - PriorityA_shots100_noise1pct_baseline_seed50.pkl\n",
      "  - PriorityA_shots100_noise1pct_XZ_seed48.pkl\n",
      "  - PriorityA_shots100_noise1pct_XZ_seed49.pkl\n",
      "  - PriorityA_shots100_noise1pct_XZ_seed50.pkl\n",
      "  - PriorityA_shots100_noise5pct_baseline_seed48.pkl\n",
      "  - PriorityA_shots100_noise5pct_baseline_seed49.pkl\n",
      "  - PriorityA_shots100_noise5pct_baseline_seed50.pkl\n",
      "  - PriorityA_shots100_noise5pct_XZ_seed48.pkl\n",
      "  - PriorityA_shots100_noise5pct_XZ_seed49.pkl\n",
      "  - PriorityA_shots100_noise5pct_XZ_seed50.pkl\n",
      "  - PriorityA_shots10_noise0pct_baseline_seed48.pkl\n",
      "  - PriorityA_shots10_noise0pct_baseline_seed49.pkl\n",
      "  - PriorityA_shots10_noise0pct_baseline_seed50.pkl\n",
      "  - PriorityA_shots10_noise0pct_XZ_seed48.pkl\n",
      "  - PriorityA_shots10_noise0pct_XZ_seed49.pkl\n",
      "  - PriorityA_shots10_noise0pct_XZ_seed50.pkl\n",
      "  - PriorityA_shots10_noise1pct_baseline_seed48.pkl\n",
      "  - PriorityA_shots10_noise1pct_baseline_seed49.pkl\n",
      "  - PriorityA_shots10_noise1pct_baseline_seed50.pkl\n",
      "  - PriorityA_shots10_noise1pct_XZ_seed48.pkl\n",
      "  - PriorityA_shots10_noise1pct_XZ_seed49.pkl\n",
      "  - PriorityA_shots10_noise1pct_XZ_seed50.pkl\n",
      "  - PriorityA_shots10_noise5pct_baseline_seed48.pkl\n",
      "  - PriorityA_shots10_noise5pct_baseline_seed49.pkl\n",
      "  - PriorityA_shots10_noise5pct_baseline_seed50.pkl\n",
      "  - PriorityA_shots10_noise5pct_XZ_seed48.pkl\n",
      "  - PriorityA_shots10_noise5pct_XZ_seed49.pkl\n",
      "  - PriorityA_shots10_noise5pct_XZ_seed50.pkl\n",
      "  - PriorityB_mixed_p0.1_baseline_seed48.pkl\n",
      "  - PriorityB_mixed_p0.1_baseline_seed49.pkl\n",
      "  - PriorityB_mixed_p0.1_baseline_seed50.pkl\n",
      "  - PriorityB_mixed_p0.1_XY_seed48.pkl\n",
      "  - PriorityB_mixed_p0.1_XY_seed49.pkl\n",
      "  - PriorityB_mixed_p0.1_XY_seed50.pkl\n",
      "  - PriorityB_mixed_p0.1_XZ_seed48.pkl\n",
      "  - PriorityB_mixed_p0.1_XZ_seed49.pkl\n",
      "  - PriorityB_mixed_p0.1_XZ_seed50.pkl\n",
      "  - PriorityB_mixed_p0.1_YZ_seed48.pkl\n",
      "  - PriorityB_mixed_p0.1_YZ_seed49.pkl\n",
      "  - PriorityB_mixed_p0.1_YZ_seed50.pkl\n",
      "  - PriorityB_mixed_p0.25_baseline_seed48.pkl\n",
      "  - PriorityB_mixed_p0.25_baseline_seed49.pkl\n",
      "  - PriorityB_mixed_p0.25_baseline_seed50.pkl\n",
      "  - PriorityB_mixed_p0.25_XY_seed48.pkl\n",
      "  - PriorityB_mixed_p0.25_XY_seed49.pkl\n",
      "  - PriorityB_mixed_p0.25_XY_seed50.pkl\n",
      "  - PriorityB_mixed_p0.25_XZ_seed48.pkl\n",
      "  - PriorityB_mixed_p0.25_XZ_seed49.pkl\n",
      "  - PriorityB_mixed_p0.25_XZ_seed50.pkl\n",
      "  - PriorityB_mixed_p0.25_YZ_seed48.pkl\n",
      "  - PriorityB_mixed_p0.25_YZ_seed49.pkl\n",
      "  - PriorityB_mixed_p0.25_YZ_seed50.pkl\n",
      "  - PriorityB_mixed_p0.5_baseline_seed48.pkl\n",
      "  - PriorityB_mixed_p0.5_baseline_seed49.pkl\n",
      "  - PriorityB_mixed_p0.5_baseline_seed50.pkl\n",
      "  - PriorityB_mixed_p0.5_XY_seed48.pkl\n",
      "  - PriorityB_mixed_p0.5_XY_seed49.pkl\n",
      "  - PriorityB_mixed_p0.5_XY_seed50.pkl\n",
      "  - PriorityB_mixed_p0.5_XZ_seed48.pkl\n",
      "  - PriorityB_mixed_p0.5_XZ_seed49.pkl\n",
      "  - PriorityB_mixed_p0.5_XZ_seed50.pkl\n",
      "  - PriorityB_mixed_p0.5_YZ_seed48.pkl\n",
      "  - PriorityB_mixed_p0.5_YZ_seed49.pkl\n",
      "  - PriorityB_mixed_p0.5_YZ_seed50.pkl\n",
      "  - PriorityB_near_pure_baseline_seed48.pkl\n",
      "  - PriorityB_near_pure_baseline_seed49.pkl\n",
      "  - PriorityB_near_pure_baseline_seed50.pkl\n",
      "  - PriorityB_near_pure_XY_seed48.pkl\n",
      "  - PriorityB_near_pure_XY_seed49.pkl\n",
      "  - PriorityB_near_pure_XY_seed50.pkl\n",
      "  - PriorityB_near_pure_XZ_seed48.pkl\n",
      "  - PriorityB_near_pure_XZ_seed49.pkl\n",
      "  - PriorityB_near_pure_XZ_seed50.pkl\n",
      "  - PriorityB_near_pure_YZ_seed48.pkl\n",
      "  - PriorityB_near_pure_YZ_seed49.pkl\n",
      "  - PriorityB_near_pure_YZ_seed50.pkl\n",
      "  - PriorityB_pure_baseline_seed48.pkl\n",
      "  - PriorityB_pure_baseline_seed49.pkl\n",
      "  - PriorityB_pure_baseline_seed50.pkl\n",
      "  - PriorityB_pure_XY_seed48.pkl\n",
      "  - PriorityB_pure_XY_seed49.pkl\n",
      "  - PriorityB_pure_XY_seed50.pkl\n",
      "  - PriorityB_pure_XZ_seed48.pkl\n",
      "  - PriorityB_pure_XZ_seed49.pkl\n",
      "  - PriorityB_pure_XZ_seed50.pkl\n",
      "  - PriorityB_pure_YZ_seed48.pkl\n",
      "  - PriorityB_pure_YZ_seed49.pkl\n",
      "  - PriorityB_pure_YZ_seed50.pkl\n",
      "  - PriorityC_sic_seed48.pkl\n",
      "  - PriorityC_sic_seed49.pkl\n",
      "  - PriorityC_sic_seed50.pkl\n",
      "  - PriorityC_XZ_seed48.pkl\n",
      "  - PriorityC_XZ_seed49.pkl\n",
      "  - PriorityC_XZ_seed50.pkl\n",
      "  - PriorityD_adaptive_baseline_seed48.pkl\n",
      "  - PriorityD_adaptive_baseline_seed49.pkl\n",
      "  - PriorityD_adaptive_baseline_seed50.pkl\n",
      "  - PriorityD_adaptive_XZ_seed48.pkl\n",
      "  - PriorityD_adaptive_XZ_seed49.pkl\n",
      "  - PriorityD_adaptive_XZ_seed50.pkl\n",
      "  - sample_training_curves_a.png\n",
      "\n",
      "================================================================================\n",
      "END OF REPORT\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ALL EXPERIMENTS COMPLETE!\n",
      "Total experiments run: 126\n",
      "Results saved to: expt_3\n",
      "================================================================================\n",
      "\n",
      "QUICK SUMMARY:\n",
      "  Priority A: 54 experiments\n",
      "  Priority B: 60 experiments\n",
      "  Priority C: 6 experiments\n",
      "  Priority D: 6 experiments\n",
      "  Total: 126 experiments\n",
      "\n",
      "  Overall Mean Fidelity: 0.8156 +/- 0.1045\n",
      "  Best Result: 0.9999\n",
      "  Worst Result: 0.5605\n"
     ]
    }
   ],
   "source": [
    "# Quick fix - redefine the function with ASCII characters\n",
    "def generate_final_report(all_results, output_dir):\n",
    "    \"\"\"Generate comprehensive final report\"\"\"\n",
    "    \n",
    "    report_path = output_dir / \"FINAL_REPORT.txt\"\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"QUANTUM STATE TOMOGRAPHY - FINAL EXPERIMENTAL REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Date: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"Master Seed: {MASTER_SEED}\\n\")\n",
    "        f.write(f\"Total Experiments: {len(all_results)}\\n\")\n",
    "        f.write(f\"Output Directory: {output_dir}\\n\\n\")\n",
    "        \n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"OVERALL STATISTICS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        all_fidelities = [r['test_fidelity_mean'] for r in all_results]\n",
    "        f.write(f\"Mean Fidelity across all experiments: {np.mean(all_fidelities):.4f} +/- {np.std(all_fidelities):.4f}\\n\")\n",
    "        f.write(f\"Best Fidelity: {np.max(all_fidelities):.4f}\\n\")\n",
    "        f.write(f\"Worst Fidelity: {np.min(all_fidelities):.4f}\\n\")\n",
    "        \n",
    "        # Count high-performing experiments\n",
    "        high_perf = sum(1 for f in all_fidelities if f > 0.90)\n",
    "        f.write(f\"\\nExperiments with fidelity > 0.90: {high_perf}/{len(all_results)} ({100*high_perf/len(all_results):.1f}%)\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "        f.write(\"KEY FINDINGS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        # Group by priority\n",
    "        priorities = {}\n",
    "        for r in all_results:\n",
    "            if 'PriorityA' in r['config']['name']:\n",
    "                priorities.setdefault('A', []).append(r)\n",
    "            elif 'PriorityB' in r['config']['name']:\n",
    "                priorities.setdefault('B', []).append(r)\n",
    "            elif 'PriorityC' in r['config']['name']:\n",
    "                priorities.setdefault('C', []).append(r)\n",
    "            elif 'PriorityD' in r['config']['name']:\n",
    "                priorities.setdefault('D', []).append(r)\n",
    "        \n",
    "        for priority, results in sorted(priorities.items()):\n",
    "            fids = [r['test_fidelity_mean'] for r in results]\n",
    "            f.write(f\"\\nPriority {priority}: {len(results)} experiments\\n\")\n",
    "            f.write(f\"  Mean Fidelity: {np.mean(fids):.4f} +/- {np.std(fids):.4f}\\n\")\n",
    "            f.write(f\"  Range: [{np.min(fids):.4f}, {np.max(fids):.4f}]\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "        f.write(\"ANOMALY CHECKS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        # Check for unexpectedly high 2-basis performance\n",
    "        two_basis_results = [r for r in all_results if any(x in r['config']['measurement_type'] \n",
    "                                                            for x in ['XY', 'XZ', 'YZ'])]\n",
    "        if two_basis_results:\n",
    "            two_basis_fids = [r['test_fidelity_mean'] for r in two_basis_results]\n",
    "            max_2basis = np.max(two_basis_fids)\n",
    "            if max_2basis > 0.90:\n",
    "                f.write(f\"[WARNING] 2-basis model achieved {max_2basis:.4f} fidelity\\n\")\n",
    "                f.write(\"    This exceeds expected performance and should be investigated.\\n\")\n",
    "            else:\n",
    "                f.write(f\"[OK] 2-basis performance reasonable (max: {max_2basis:.4f})\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "        f.write(\"FILES GENERATED\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        files = list(output_dir.glob(\"*\"))\n",
    "        for file in sorted(files):\n",
    "            f.write(f\"  - {file.name}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        f.write(\"END OF REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nFinal report saved to: {report_path}\")\n",
    "    \n",
    "    # Also print to console\n",
    "    with open(report_path, 'r', encoding='utf-8') as f:\n",
    "        print(f.read())\n",
    "\n",
    "# Now run it\n",
    "generate_final_report(all_results, OUTPUT_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL EXPERIMENTS COMPLETE!\")\n",
    "print(f\"Total experiments run: {len(all_results)}\")\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nQUICK SUMMARY:\")\n",
    "print(f\"  Priority A: {len(results_a)} experiments\")\n",
    "print(f\"  Priority B: {len(results_b)} experiments\")\n",
    "print(f\"  Priority C: {len(results_c)} experiments\")\n",
    "print(f\"  Priority D: {len(results_d)} experiments\")\n",
    "print(f\"  Total: {len(all_results)} experiments\")\n",
    "\n",
    "all_fids = [r['test_fidelity_mean'] for r in all_results]\n",
    "print(f\"\\n  Overall Mean Fidelity: {np.mean(all_fids):.4f} +/- {np.std(all_fids):.4f}\")\n",
    "print(f\"  Best Result: {np.max(all_fids):.4f}\")\n",
    "print(f\"  Worst Result: {np.min(all_fids):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum-tomography-gd0kJOpx-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
