{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8980a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Results Analysis\n",
    "\n",
    "Analyze all experimental results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load results\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = Path(\"../results/expt_3\")\n",
    "\n",
    "# Load all results\n",
    "all_results = []\n",
    "for pkl_file in RESULTS_DIR.glob(\"*.pkl\"):\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        all_results.append(pickle.load(f))\n",
    "\n",
    "print(f\"Loaded {len(all_results)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Overall statistics\n",
    "fidelities = [r['test_fidelity_mean'] for r in all_results]\n",
    "\n",
    "print(f\"Overall Statistics:\")\n",
    "print(f\"  Mean Fidelity: {np.mean(fidelities):.4f} ± {np.std(fidelities):.4f}\")\n",
    "print(f\"  Best: {np.max(fidelities):.4f}\")\n",
    "print(f\"  Worst: {np.min(fidelities):.4f}\")\n",
    "print(f\"  Experiments > 0.90: {sum(f > 0.90 for f in fidelities)}/{len(fidelities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d267eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Group by priority\n",
    "priorities = {'A': [], 'B': [], 'C': [], 'D': []}\n",
    "for r in all_results:\n",
    "    if 'PriorityA' in r['config']['name']:\n",
    "        priorities['A'].append(r)\n",
    "    elif 'PriorityB' in r['config']['name']:\n",
    "        priorities['B'].append(r)\n",
    "    elif 'PriorityC' in r['config']['name']:\n",
    "        priorities['C'].append(r)\n",
    "    elif 'PriorityD' in r['config']['name']:\n",
    "        priorities['D'].append(r)\n",
    "\n",
    "for priority, results in priorities.items():\n",
    "    fids = [r['test_fidelity_mean'] for r in results]\n",
    "    print(f\"\\nPriority {priority}: {len(results)} experiments\")\n",
    "    print(f\"  Mean: {np.mean(fids):.4f} ± {np.std(fids):.4f}\")\n",
    "    print(f\"  Range: [{np.min(fids):.4f}, {np.max(fids):.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Analyze by measurement type\n",
    "by_measurement = {}\n",
    "for r in all_results:\n",
    "    meas = r['config']['measurement_type']\n",
    "    by_measurement.setdefault(meas, []).append(r['test_fidelity_mean'])\n",
    "\n",
    "print(\"\\nBy Measurement Type:\")\n",
    "for meas, fids in sorted(by_measurement.items()):\n",
    "    print(f\"  {meas:10s}: {np.mean(fids):.4f} ± {np.std(fids):.4f} (n={len(fids)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3192003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Priority A - Shots vs Performance\n",
    "priority_a = priorities['A']\n",
    "\n",
    "shots_data = {}\n",
    "for r in priority_a:\n",
    "    if 'shots' in r['config']:\n",
    "        key = (r['config']['shots'], r['config']['noise_level'], r['config']['measurement_type'])\n",
    "        shots_data.setdefault(key, []).append(r['test_fidelity_mean'])\n",
    "\n",
    "# Create plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for noise in [0.0, 0.01, 0.05]:\n",
    "    shots = []\n",
    "    baseline_fids = []\n",
    "    xz_fids = []\n",
    "    \n",
    "    for (s, n, m), fids in shots_data.items():\n",
    "        if n == noise:\n",
    "            if m == 'baseline':\n",
    "                shots.append(s)\n",
    "                baseline_fids.append(np.mean(fids))\n",
    "            elif m == 'XZ':\n",
    "                xz_fids.append(np.mean(fids))\n",
    "    \n",
    "    if shots:\n",
    "        axes[0].plot(shots, baseline_fids, 'o-', label=f'Baseline, {int(noise*100)}% noise')\n",
    "        axes[1].plot(shots, xz_fids, 's-', label=f'XZ, {int(noise*100)}% noise')\n",
    "\n",
    "axes[0].set_xlabel('Shots')\n",
    "axes[0].set_ylabel('Mean Fidelity')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_title('Baseline Performance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Shots')\n",
    "axes[1].set_ylabel('Mean Fidelity')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_title('XZ Two-Basis Performance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shots_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f35c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Priority B - Ensemble analysis\n",
    "priority_b = priorities['B']\n",
    "\n",
    "ensemble_data = {}\n",
    "for r in priority_b:\n",
    "    ensemble = r['config']['ensemble_type']\n",
    "    mixing = r['config'].get('mixing_p')\n",
    "    key = f\"{ensemble}_p{mixing}\" if mixing else ensemble\n",
    "    ensemble_data.setdefault(key, []).append(r['test_fidelity_mean'])\n",
    "\n",
    "print(\"\\nEnsemble Performance:\")\n",
    "for ensemble, fids in sorted(ensemble_data.items()):\n",
    "    print(f\"  {ensemble:20s}: {np.mean(fids):.4f} ± {np.std(fids):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd89ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Best and worst experiments\n",
    "best_exp = max(all_results, key=lambda r: r['test_fidelity_mean'])\n",
    "worst_exp = min(all_results, key=lambda r: r['test_fidelity_mean'])\n",
    "\n",
    "print(\"\\nBest Experiment:\")\n",
    "print(f\"  Name: {best_exp['config']['name']}\")\n",
    "print(f\"  Fidelity: {best_exp['test_fidelity_mean']:.4f}\")\n",
    "\n",
    "print(\"\\nWorst Experiment:\")\n",
    "print(f\"  Name: {worst_exp['config']['name']}\")\n",
    "print(f\"  Fidelity: {worst_exp['test_fidelity_mean']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum-tomography-gd0kJOpx-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
